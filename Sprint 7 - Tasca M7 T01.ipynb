{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72d0bedd",
   "metadata": {},
   "source": [
    "- Exercici 1\n",
    "Crea almenys dos models de classificació diferents per intentar predir el millor les classes de l'arxiu adjunt.\n",
    "\n",
    "- Exercici 2\n",
    "Compara els models de classificació utilitzant la precisió (accuracy), una matriu de confusió i d’altres mètriques més avançades.\n",
    "\n",
    "- Exercici 3\n",
    "Entrena’ls usant els diferents paràmetres que admeten per tal de millorar-ne la predicció.\n",
    "\n",
    "- Exercici 4\n",
    "Compara el seu rendiment fent servir l’aproximació traint/test o cross-validation.\n",
    "\n",
    "- Exercici 5\n",
    "Aplica algun procés d'enginyeria per millorar els resultats (normalització, estandardització, mostreig...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1215d190",
   "metadata": {},
   "source": [
    "### Previa:\n",
    "\n",
    "- Carreguem les dades a analitzar (wineData.txt)\n",
    "- El dataset proporciona les dades obtingudes dels analisis de vins d'una mateixa regió d'Italia procedents de 3 cultius diferents.\n",
    "- Le dades obtingudes corresponen a 13 constituyents que es troben a cada un dels 3 tipus de vins.\n",
    "- L'objectiu serà predir el tipus de vi en funció d'on ha estat cultivat. En funció d'on han estat cultivats es classifiquen en 3 clases -- 'Class': 1, 2 o 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92bb14ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.640000</td>\n",
       "      <td>1.040</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.380000</td>\n",
       "      <td>1.050</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.680000</td>\n",
       "      <td>1.030</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>0.860</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.320000</td>\n",
       "      <td>1.040</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>14.20</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.45</td>\n",
       "      <td>15.2</td>\n",
       "      <td>112</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.97</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>1.050</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>14.39</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.45</td>\n",
       "      <td>14.6</td>\n",
       "      <td>96</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>1.020</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>14.06</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.61</td>\n",
       "      <td>17.6</td>\n",
       "      <td>121</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.050000</td>\n",
       "      <td>1.060</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>14.83</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.17</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>1.080</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>13.86</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.27</td>\n",
       "      <td>16.0</td>\n",
       "      <td>98</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.85</td>\n",
       "      <td>7.220000</td>\n",
       "      <td>1.010</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>14.10</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.30</td>\n",
       "      <td>18.0</td>\n",
       "      <td>105</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>14.12</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.32</td>\n",
       "      <td>16.8</td>\n",
       "      <td>95</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.57</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.170</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>13.75</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.41</td>\n",
       "      <td>16.0</td>\n",
       "      <td>89</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.81</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>1.150</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>14.75</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.39</td>\n",
       "      <td>11.4</td>\n",
       "      <td>91</td>\n",
       "      <td>3.10</td>\n",
       "      <td>3.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>2.73</td>\n",
       "      <td>1150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>14.38</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.38</td>\n",
       "      <td>12.0</td>\n",
       "      <td>102</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.96</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>1.200</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>13.63</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.70</td>\n",
       "      <td>17.2</td>\n",
       "      <td>112</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.91</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.46</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>1.280</td>\n",
       "      <td>2.88</td>\n",
       "      <td>1310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>14.30</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2.72</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.97</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>1.070</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>13.83</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.62</td>\n",
       "      <td>20.0</td>\n",
       "      <td>115</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.72</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>1.130</td>\n",
       "      <td>2.57</td>\n",
       "      <td>1130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>14.19</td>\n",
       "      <td>1.59</td>\n",
       "      <td>2.48</td>\n",
       "      <td>16.5</td>\n",
       "      <td>108</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.86</td>\n",
       "      <td>8.700000</td>\n",
       "      <td>1.230</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>13.64</td>\n",
       "      <td>3.10</td>\n",
       "      <td>2.56</td>\n",
       "      <td>15.2</td>\n",
       "      <td>116</td>\n",
       "      <td>2.70</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.66</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>0.960</td>\n",
       "      <td>3.36</td>\n",
       "      <td>845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>14.06</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.28</td>\n",
       "      <td>16.0</td>\n",
       "      <td>126</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.10</td>\n",
       "      <td>5.650000</td>\n",
       "      <td>1.090</td>\n",
       "      <td>3.71</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>12.93</td>\n",
       "      <td>3.80</td>\n",
       "      <td>2.65</td>\n",
       "      <td>18.6</td>\n",
       "      <td>102</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.030</td>\n",
       "      <td>3.52</td>\n",
       "      <td>770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>13.71</td>\n",
       "      <td>1.86</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.61</td>\n",
       "      <td>2.88</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.69</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>1.110</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>12.85</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.52</td>\n",
       "      <td>17.8</td>\n",
       "      <td>95</td>\n",
       "      <td>2.48</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.46</td>\n",
       "      <td>3.930000</td>\n",
       "      <td>1.090</td>\n",
       "      <td>3.63</td>\n",
       "      <td>1015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>13.50</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.61</td>\n",
       "      <td>20.0</td>\n",
       "      <td>96</td>\n",
       "      <td>2.53</td>\n",
       "      <td>2.61</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.520000</td>\n",
       "      <td>1.120</td>\n",
       "      <td>3.82</td>\n",
       "      <td>845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>13.05</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.22</td>\n",
       "      <td>25.0</td>\n",
       "      <td>124</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3.580000</td>\n",
       "      <td>1.130</td>\n",
       "      <td>3.20</td>\n",
       "      <td>830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>13.39</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.62</td>\n",
       "      <td>16.1</td>\n",
       "      <td>93</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.94</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.45</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>0.920</td>\n",
       "      <td>3.22</td>\n",
       "      <td>1195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>13.30</td>\n",
       "      <td>1.72</td>\n",
       "      <td>2.14</td>\n",
       "      <td>17.0</td>\n",
       "      <td>94</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.35</td>\n",
       "      <td>3.950000</td>\n",
       "      <td>1.020</td>\n",
       "      <td>2.77</td>\n",
       "      <td>1285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>13.87</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.80</td>\n",
       "      <td>19.4</td>\n",
       "      <td>107</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.97</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.76</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>3.40</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>14.02</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.21</td>\n",
       "      <td>16.0</td>\n",
       "      <td>96</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.98</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>1.040</td>\n",
       "      <td>3.59</td>\n",
       "      <td>1035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>13.73</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.70</td>\n",
       "      <td>22.5</td>\n",
       "      <td>101</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>1.190</td>\n",
       "      <td>2.71</td>\n",
       "      <td>1285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>13.58</td>\n",
       "      <td>1.66</td>\n",
       "      <td>2.36</td>\n",
       "      <td>19.1</td>\n",
       "      <td>106</td>\n",
       "      <td>2.86</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.95</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>1.090</td>\n",
       "      <td>2.88</td>\n",
       "      <td>1515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>13.68</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2.36</td>\n",
       "      <td>17.2</td>\n",
       "      <td>104</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.97</td>\n",
       "      <td>3.840000</td>\n",
       "      <td>1.230</td>\n",
       "      <td>2.87</td>\n",
       "      <td>990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>13.76</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.70</td>\n",
       "      <td>19.5</td>\n",
       "      <td>132</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.74</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.35</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>13.51</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.65</td>\n",
       "      <td>19.0</td>\n",
       "      <td>110</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.53</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.54</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>1.100</td>\n",
       "      <td>2.87</td>\n",
       "      <td>1095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>13.48</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.41</td>\n",
       "      <td>20.5</td>\n",
       "      <td>100</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.86</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.040</td>\n",
       "      <td>3.47</td>\n",
       "      <td>920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>13.28</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.84</td>\n",
       "      <td>15.5</td>\n",
       "      <td>110</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.36</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>1.090</td>\n",
       "      <td>2.78</td>\n",
       "      <td>880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>13.05</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.55</td>\n",
       "      <td>18.0</td>\n",
       "      <td>98</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.44</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>1.120</td>\n",
       "      <td>2.51</td>\n",
       "      <td>1105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>13.07</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.10</td>\n",
       "      <td>15.5</td>\n",
       "      <td>98</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.37</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1.180</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>14.22</td>\n",
       "      <td>3.99</td>\n",
       "      <td>2.51</td>\n",
       "      <td>13.2</td>\n",
       "      <td>128</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.08</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>0.890</td>\n",
       "      <td>3.53</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>13.56</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.31</td>\n",
       "      <td>16.2</td>\n",
       "      <td>117</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.34</td>\n",
       "      <td>2.34</td>\n",
       "      <td>6.130000</td>\n",
       "      <td>0.950</td>\n",
       "      <td>3.38</td>\n",
       "      <td>795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>13.41</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.12</td>\n",
       "      <td>18.8</td>\n",
       "      <td>90</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.48</td>\n",
       "      <td>4.280000</td>\n",
       "      <td>0.910</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>13.88</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.59</td>\n",
       "      <td>15.0</td>\n",
       "      <td>101</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.70</td>\n",
       "      <td>5.430000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>3.56</td>\n",
       "      <td>1095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.29</td>\n",
       "      <td>17.5</td>\n",
       "      <td>103</td>\n",
       "      <td>2.64</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4.360000</td>\n",
       "      <td>0.820</td>\n",
       "      <td>3.00</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>13.05</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.10</td>\n",
       "      <td>17.0</td>\n",
       "      <td>107</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.03</td>\n",
       "      <td>5.040000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>3.35</td>\n",
       "      <td>885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>14.21</td>\n",
       "      <td>4.04</td>\n",
       "      <td>2.44</td>\n",
       "      <td>18.9</td>\n",
       "      <td>111</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.240000</td>\n",
       "      <td>0.870</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>14.38</td>\n",
       "      <td>3.59</td>\n",
       "      <td>2.28</td>\n",
       "      <td>16.0</td>\n",
       "      <td>102</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.27</td>\n",
       "      <td>2.19</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>1.040</td>\n",
       "      <td>3.44</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>13.90</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.12</td>\n",
       "      <td>16.0</td>\n",
       "      <td>101</td>\n",
       "      <td>3.10</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2.14</td>\n",
       "      <td>6.100000</td>\n",
       "      <td>0.910</td>\n",
       "      <td>3.33</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>14.10</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.40</td>\n",
       "      <td>18.8</td>\n",
       "      <td>103</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2.92</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2.38</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>1.070</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>13.94</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.27</td>\n",
       "      <td>17.4</td>\n",
       "      <td>108</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2.08</td>\n",
       "      <td>8.900000</td>\n",
       "      <td>1.120</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>13.05</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.04</td>\n",
       "      <td>12.4</td>\n",
       "      <td>92</td>\n",
       "      <td>2.72</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.17</td>\n",
       "      <td>2.91</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>1.120</td>\n",
       "      <td>2.91</td>\n",
       "      <td>1150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>13.83</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.60</td>\n",
       "      <td>17.2</td>\n",
       "      <td>94</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>1.240</td>\n",
       "      <td>3.37</td>\n",
       "      <td>1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>13.82</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.42</td>\n",
       "      <td>14.0</td>\n",
       "      <td>111</td>\n",
       "      <td>3.88</td>\n",
       "      <td>3.74</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.87</td>\n",
       "      <td>7.050000</td>\n",
       "      <td>1.010</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>13.77</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.68</td>\n",
       "      <td>17.1</td>\n",
       "      <td>115</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.79</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.68</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>1.130</td>\n",
       "      <td>2.93</td>\n",
       "      <td>1375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>13.74</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.25</td>\n",
       "      <td>16.4</td>\n",
       "      <td>118</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.62</td>\n",
       "      <td>5.850000</td>\n",
       "      <td>0.920</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>13.56</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.46</td>\n",
       "      <td>20.5</td>\n",
       "      <td>116</td>\n",
       "      <td>2.96</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.45</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>0.980</td>\n",
       "      <td>3.03</td>\n",
       "      <td>1120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>14.22</td>\n",
       "      <td>1.70</td>\n",
       "      <td>2.30</td>\n",
       "      <td>16.3</td>\n",
       "      <td>118</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>2.03</td>\n",
       "      <td>6.380000</td>\n",
       "      <td>0.940</td>\n",
       "      <td>3.31</td>\n",
       "      <td>970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "      <td>13.29</td>\n",
       "      <td>1.97</td>\n",
       "      <td>2.68</td>\n",
       "      <td>16.8</td>\n",
       "      <td>102</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.66</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.070</td>\n",
       "      <td>2.84</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>13.72</td>\n",
       "      <td>1.43</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.7</td>\n",
       "      <td>108</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0.19</td>\n",
       "      <td>2.04</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.890</td>\n",
       "      <td>2.87</td>\n",
       "      <td>1285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2</td>\n",
       "      <td>12.37</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.36</td>\n",
       "      <td>10.6</td>\n",
       "      <td>88</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>1.050</td>\n",
       "      <td>1.82</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2</td>\n",
       "      <td>12.33</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2.28</td>\n",
       "      <td>16.0</td>\n",
       "      <td>101</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.41</td>\n",
       "      <td>3.270000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>1.67</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2</td>\n",
       "      <td>12.64</td>\n",
       "      <td>1.36</td>\n",
       "      <td>2.02</td>\n",
       "      <td>16.8</td>\n",
       "      <td>100</td>\n",
       "      <td>2.02</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.62</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>0.980</td>\n",
       "      <td>1.59</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2</td>\n",
       "      <td>13.67</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.92</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.73</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>1.230</td>\n",
       "      <td>2.46</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2</td>\n",
       "      <td>12.37</td>\n",
       "      <td>1.13</td>\n",
       "      <td>2.16</td>\n",
       "      <td>19.0</td>\n",
       "      <td>87</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.87</td>\n",
       "      <td>4.450000</td>\n",
       "      <td>1.220</td>\n",
       "      <td>2.87</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2</td>\n",
       "      <td>12.17</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.53</td>\n",
       "      <td>19.0</td>\n",
       "      <td>104</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.03</td>\n",
       "      <td>2.950000</td>\n",
       "      <td>1.450</td>\n",
       "      <td>2.23</td>\n",
       "      <td>355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2</td>\n",
       "      <td>12.37</td>\n",
       "      <td>1.21</td>\n",
       "      <td>2.56</td>\n",
       "      <td>18.1</td>\n",
       "      <td>98</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.37</td>\n",
       "      <td>2.08</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>1.190</td>\n",
       "      <td>2.30</td>\n",
       "      <td>678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2</td>\n",
       "      <td>13.11</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.70</td>\n",
       "      <td>15.0</td>\n",
       "      <td>78</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.26</td>\n",
       "      <td>2.28</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>1.120</td>\n",
       "      <td>3.18</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2</td>\n",
       "      <td>12.37</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.92</td>\n",
       "      <td>19.6</td>\n",
       "      <td>78</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.04</td>\n",
       "      <td>4.680000</td>\n",
       "      <td>1.120</td>\n",
       "      <td>3.48</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2</td>\n",
       "      <td>13.34</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2.36</td>\n",
       "      <td>17.0</td>\n",
       "      <td>110</td>\n",
       "      <td>2.53</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.42</td>\n",
       "      <td>3.170000</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.93</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2</td>\n",
       "      <td>12.21</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.75</td>\n",
       "      <td>16.8</td>\n",
       "      <td>151</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.850000</td>\n",
       "      <td>1.280</td>\n",
       "      <td>3.07</td>\n",
       "      <td>718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2</td>\n",
       "      <td>12.29</td>\n",
       "      <td>1.61</td>\n",
       "      <td>2.21</td>\n",
       "      <td>20.4</td>\n",
       "      <td>103</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.46</td>\n",
       "      <td>3.050000</td>\n",
       "      <td>0.906</td>\n",
       "      <td>1.82</td>\n",
       "      <td>870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2</td>\n",
       "      <td>13.86</td>\n",
       "      <td>1.51</td>\n",
       "      <td>2.67</td>\n",
       "      <td>25.0</td>\n",
       "      <td>86</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.87</td>\n",
       "      <td>3.380000</td>\n",
       "      <td>1.360</td>\n",
       "      <td>3.16</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2</td>\n",
       "      <td>13.49</td>\n",
       "      <td>1.66</td>\n",
       "      <td>2.24</td>\n",
       "      <td>24.0</td>\n",
       "      <td>87</td>\n",
       "      <td>1.88</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.740000</td>\n",
       "      <td>0.980</td>\n",
       "      <td>2.78</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2</td>\n",
       "      <td>12.99</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.60</td>\n",
       "      <td>30.0</td>\n",
       "      <td>139</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.96</td>\n",
       "      <td>3.350000</td>\n",
       "      <td>1.310</td>\n",
       "      <td>3.50</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2</td>\n",
       "      <td>11.96</td>\n",
       "      <td>1.09</td>\n",
       "      <td>2.30</td>\n",
       "      <td>21.0</td>\n",
       "      <td>101</td>\n",
       "      <td>3.38</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.65</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.990</td>\n",
       "      <td>3.13</td>\n",
       "      <td>886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2</td>\n",
       "      <td>11.66</td>\n",
       "      <td>1.88</td>\n",
       "      <td>1.92</td>\n",
       "      <td>16.0</td>\n",
       "      <td>97</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.15</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>1.230</td>\n",
       "      <td>2.14</td>\n",
       "      <td>428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2</td>\n",
       "      <td>13.03</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.71</td>\n",
       "      <td>16.0</td>\n",
       "      <td>86</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.46</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>1.190</td>\n",
       "      <td>2.48</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2</td>\n",
       "      <td>11.84</td>\n",
       "      <td>2.89</td>\n",
       "      <td>2.23</td>\n",
       "      <td>18.0</td>\n",
       "      <td>112</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>0.960</td>\n",
       "      <td>2.52</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2</td>\n",
       "      <td>12.33</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.95</td>\n",
       "      <td>14.8</td>\n",
       "      <td>136</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.35</td>\n",
       "      <td>2.76</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>1.060</td>\n",
       "      <td>2.31</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2</td>\n",
       "      <td>12.70</td>\n",
       "      <td>3.87</td>\n",
       "      <td>2.40</td>\n",
       "      <td>23.0</td>\n",
       "      <td>101</td>\n",
       "      <td>2.83</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.570000</td>\n",
       "      <td>1.190</td>\n",
       "      <td>3.13</td>\n",
       "      <td>463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2</td>\n",
       "      <td>12.00</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2.00</td>\n",
       "      <td>19.0</td>\n",
       "      <td>86</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.26</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.43</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.380</td>\n",
       "      <td>3.12</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2</td>\n",
       "      <td>12.72</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.20</td>\n",
       "      <td>18.8</td>\n",
       "      <td>86</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.53</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.77</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>1.160</td>\n",
       "      <td>3.14</td>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2</td>\n",
       "      <td>12.08</td>\n",
       "      <td>1.13</td>\n",
       "      <td>2.51</td>\n",
       "      <td>24.0</td>\n",
       "      <td>78</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1.310</td>\n",
       "      <td>2.72</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2</td>\n",
       "      <td>13.05</td>\n",
       "      <td>3.86</td>\n",
       "      <td>2.32</td>\n",
       "      <td>22.5</td>\n",
       "      <td>85</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.62</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>0.840</td>\n",
       "      <td>2.01</td>\n",
       "      <td>515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2</td>\n",
       "      <td>11.84</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.58</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.35</td>\n",
       "      <td>3.050000</td>\n",
       "      <td>0.790</td>\n",
       "      <td>3.08</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2</td>\n",
       "      <td>12.67</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2.24</td>\n",
       "      <td>18.0</td>\n",
       "      <td>99</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.46</td>\n",
       "      <td>2.620000</td>\n",
       "      <td>1.230</td>\n",
       "      <td>3.16</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2</td>\n",
       "      <td>12.16</td>\n",
       "      <td>1.61</td>\n",
       "      <td>2.31</td>\n",
       "      <td>22.8</td>\n",
       "      <td>90</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.56</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>1.330</td>\n",
       "      <td>2.26</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2</td>\n",
       "      <td>11.65</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.62</td>\n",
       "      <td>26.0</td>\n",
       "      <td>88</td>\n",
       "      <td>1.92</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>1.360</td>\n",
       "      <td>3.21</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2</td>\n",
       "      <td>11.64</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.46</td>\n",
       "      <td>21.6</td>\n",
       "      <td>84</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.75</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2</td>\n",
       "      <td>12.08</td>\n",
       "      <td>1.33</td>\n",
       "      <td>2.30</td>\n",
       "      <td>23.6</td>\n",
       "      <td>70</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>1.070</td>\n",
       "      <td>3.21</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>2</td>\n",
       "      <td>12.08</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2.32</td>\n",
       "      <td>18.5</td>\n",
       "      <td>81</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1.080</td>\n",
       "      <td>2.27</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2</td>\n",
       "      <td>12.00</td>\n",
       "      <td>1.51</td>\n",
       "      <td>2.42</td>\n",
       "      <td>22.0</td>\n",
       "      <td>86</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.63</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>1.050</td>\n",
       "      <td>2.65</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2</td>\n",
       "      <td>12.69</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.7</td>\n",
       "      <td>80</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.62</td>\n",
       "      <td>3.050000</td>\n",
       "      <td>0.960</td>\n",
       "      <td>2.06</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>2</td>\n",
       "      <td>12.29</td>\n",
       "      <td>2.83</td>\n",
       "      <td>2.22</td>\n",
       "      <td>18.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>1.150</td>\n",
       "      <td>3.30</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2</td>\n",
       "      <td>11.62</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.28</td>\n",
       "      <td>18.0</td>\n",
       "      <td>98</td>\n",
       "      <td>3.02</td>\n",
       "      <td>2.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.35</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>1.160</td>\n",
       "      <td>2.96</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2</td>\n",
       "      <td>12.47</td>\n",
       "      <td>1.52</td>\n",
       "      <td>2.20</td>\n",
       "      <td>19.0</td>\n",
       "      <td>162</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.32</td>\n",
       "      <td>3.28</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>1.160</td>\n",
       "      <td>2.63</td>\n",
       "      <td>937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2</td>\n",
       "      <td>11.81</td>\n",
       "      <td>2.12</td>\n",
       "      <td>2.74</td>\n",
       "      <td>21.5</td>\n",
       "      <td>134</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.56</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.950</td>\n",
       "      <td>2.26</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2</td>\n",
       "      <td>12.29</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.98</td>\n",
       "      <td>16.0</td>\n",
       "      <td>85</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.230</td>\n",
       "      <td>2.74</td>\n",
       "      <td>428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2</td>\n",
       "      <td>12.37</td>\n",
       "      <td>1.07</td>\n",
       "      <td>2.10</td>\n",
       "      <td>18.5</td>\n",
       "      <td>88</td>\n",
       "      <td>3.52</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.95</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.040</td>\n",
       "      <td>2.77</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2</td>\n",
       "      <td>12.29</td>\n",
       "      <td>3.17</td>\n",
       "      <td>2.21</td>\n",
       "      <td>18.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1.420</td>\n",
       "      <td>2.83</td>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2</td>\n",
       "      <td>12.08</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1.70</td>\n",
       "      <td>17.5</td>\n",
       "      <td>97</td>\n",
       "      <td>2.23</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.40</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>1.270</td>\n",
       "      <td>2.96</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2</td>\n",
       "      <td>12.60</td>\n",
       "      <td>1.34</td>\n",
       "      <td>1.90</td>\n",
       "      <td>18.5</td>\n",
       "      <td>88</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>1.040</td>\n",
       "      <td>2.77</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2</td>\n",
       "      <td>12.34</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.46</td>\n",
       "      <td>21.0</td>\n",
       "      <td>98</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.31</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>0.800</td>\n",
       "      <td>3.38</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2</td>\n",
       "      <td>11.82</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.88</td>\n",
       "      <td>19.5</td>\n",
       "      <td>86</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2.060000</td>\n",
       "      <td>0.940</td>\n",
       "      <td>2.44</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2</td>\n",
       "      <td>12.51</td>\n",
       "      <td>1.73</td>\n",
       "      <td>1.98</td>\n",
       "      <td>20.5</td>\n",
       "      <td>85</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.940000</td>\n",
       "      <td>1.040</td>\n",
       "      <td>3.57</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2</td>\n",
       "      <td>12.42</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.27</td>\n",
       "      <td>22.0</td>\n",
       "      <td>90</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>0.860</td>\n",
       "      <td>3.30</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2</td>\n",
       "      <td>12.25</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.12</td>\n",
       "      <td>19.0</td>\n",
       "      <td>80</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.63</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.17</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2</td>\n",
       "      <td>12.72</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.28</td>\n",
       "      <td>22.5</td>\n",
       "      <td>84</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.63</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>2.42</td>\n",
       "      <td>488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2</td>\n",
       "      <td>12.22</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.94</td>\n",
       "      <td>19.0</td>\n",
       "      <td>92</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.39</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>0.860</td>\n",
       "      <td>3.02</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2</td>\n",
       "      <td>11.61</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.70</td>\n",
       "      <td>20.0</td>\n",
       "      <td>94</td>\n",
       "      <td>2.74</td>\n",
       "      <td>2.92</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.49</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>0.960</td>\n",
       "      <td>3.26</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2</td>\n",
       "      <td>11.46</td>\n",
       "      <td>3.74</td>\n",
       "      <td>1.82</td>\n",
       "      <td>19.5</td>\n",
       "      <td>107</td>\n",
       "      <td>3.18</td>\n",
       "      <td>2.58</td>\n",
       "      <td>0.24</td>\n",
       "      <td>3.58</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>0.750</td>\n",
       "      <td>2.81</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2</td>\n",
       "      <td>12.52</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.17</td>\n",
       "      <td>21.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.22</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.900</td>\n",
       "      <td>2.78</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>2</td>\n",
       "      <td>11.76</td>\n",
       "      <td>2.68</td>\n",
       "      <td>2.92</td>\n",
       "      <td>20.0</td>\n",
       "      <td>103</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>1.230</td>\n",
       "      <td>2.50</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2</td>\n",
       "      <td>11.41</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2.50</td>\n",
       "      <td>21.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2.48</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.44</td>\n",
       "      <td>3.080000</td>\n",
       "      <td>1.100</td>\n",
       "      <td>2.31</td>\n",
       "      <td>434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>2</td>\n",
       "      <td>12.08</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.50</td>\n",
       "      <td>22.5</td>\n",
       "      <td>84</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>0.930</td>\n",
       "      <td>3.19</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2</td>\n",
       "      <td>11.03</td>\n",
       "      <td>1.51</td>\n",
       "      <td>2.20</td>\n",
       "      <td>21.5</td>\n",
       "      <td>85</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>1.710</td>\n",
       "      <td>2.87</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2</td>\n",
       "      <td>11.82</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.99</td>\n",
       "      <td>20.8</td>\n",
       "      <td>86</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>0.950</td>\n",
       "      <td>3.33</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2</td>\n",
       "      <td>12.42</td>\n",
       "      <td>1.61</td>\n",
       "      <td>2.19</td>\n",
       "      <td>22.5</td>\n",
       "      <td>108</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.61</td>\n",
       "      <td>2.060000</td>\n",
       "      <td>1.060</td>\n",
       "      <td>2.96</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>2</td>\n",
       "      <td>12.77</td>\n",
       "      <td>3.43</td>\n",
       "      <td>1.98</td>\n",
       "      <td>16.0</td>\n",
       "      <td>80</td>\n",
       "      <td>1.63</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.83</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>2.12</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2</td>\n",
       "      <td>12.00</td>\n",
       "      <td>3.43</td>\n",
       "      <td>2.00</td>\n",
       "      <td>19.0</td>\n",
       "      <td>87</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>0.930</td>\n",
       "      <td>3.05</td>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2</td>\n",
       "      <td>11.45</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.42</td>\n",
       "      <td>20.0</td>\n",
       "      <td>96</td>\n",
       "      <td>2.90</td>\n",
       "      <td>2.79</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.83</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.800</td>\n",
       "      <td>3.39</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2</td>\n",
       "      <td>11.56</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.23</td>\n",
       "      <td>28.5</td>\n",
       "      <td>119</td>\n",
       "      <td>3.18</td>\n",
       "      <td>5.08</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.87</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.930</td>\n",
       "      <td>3.69</td>\n",
       "      <td>465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2</td>\n",
       "      <td>12.42</td>\n",
       "      <td>4.43</td>\n",
       "      <td>2.73</td>\n",
       "      <td>26.5</td>\n",
       "      <td>102</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.080000</td>\n",
       "      <td>0.920</td>\n",
       "      <td>3.12</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2</td>\n",
       "      <td>13.05</td>\n",
       "      <td>5.80</td>\n",
       "      <td>2.13</td>\n",
       "      <td>21.5</td>\n",
       "      <td>86</td>\n",
       "      <td>2.62</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.730</td>\n",
       "      <td>3.10</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2</td>\n",
       "      <td>11.87</td>\n",
       "      <td>4.31</td>\n",
       "      <td>2.39</td>\n",
       "      <td>21.0</td>\n",
       "      <td>82</td>\n",
       "      <td>2.86</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2.91</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>0.750</td>\n",
       "      <td>3.64</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2</td>\n",
       "      <td>12.07</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.17</td>\n",
       "      <td>21.0</td>\n",
       "      <td>85</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.760000</td>\n",
       "      <td>0.860</td>\n",
       "      <td>3.28</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2</td>\n",
       "      <td>12.43</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.29</td>\n",
       "      <td>21.5</td>\n",
       "      <td>86</td>\n",
       "      <td>2.74</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.77</td>\n",
       "      <td>3.940000</td>\n",
       "      <td>0.690</td>\n",
       "      <td>2.84</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2</td>\n",
       "      <td>11.79</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.78</td>\n",
       "      <td>28.5</td>\n",
       "      <td>92</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.76</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.970</td>\n",
       "      <td>2.44</td>\n",
       "      <td>466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2</td>\n",
       "      <td>12.37</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.30</td>\n",
       "      <td>24.5</td>\n",
       "      <td>88</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.120000</td>\n",
       "      <td>0.890</td>\n",
       "      <td>2.78</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2</td>\n",
       "      <td>12.04</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.38</td>\n",
       "      <td>22.0</td>\n",
       "      <td>80</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.790</td>\n",
       "      <td>2.57</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>3</td>\n",
       "      <td>12.86</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.32</td>\n",
       "      <td>18.0</td>\n",
       "      <td>122</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.94</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>0.760</td>\n",
       "      <td>1.29</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>3</td>\n",
       "      <td>12.88</td>\n",
       "      <td>2.99</td>\n",
       "      <td>2.40</td>\n",
       "      <td>20.0</td>\n",
       "      <td>104</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.83</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>0.740</td>\n",
       "      <td>1.42</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>3</td>\n",
       "      <td>12.81</td>\n",
       "      <td>2.31</td>\n",
       "      <td>2.40</td>\n",
       "      <td>24.0</td>\n",
       "      <td>98</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.83</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>0.660</td>\n",
       "      <td>1.36</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>3</td>\n",
       "      <td>12.70</td>\n",
       "      <td>3.55</td>\n",
       "      <td>2.36</td>\n",
       "      <td>21.5</td>\n",
       "      <td>106</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.84</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.780</td>\n",
       "      <td>1.29</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>3</td>\n",
       "      <td>12.51</td>\n",
       "      <td>1.24</td>\n",
       "      <td>2.25</td>\n",
       "      <td>17.5</td>\n",
       "      <td>85</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.450000</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.51</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>3</td>\n",
       "      <td>12.60</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.20</td>\n",
       "      <td>18.5</td>\n",
       "      <td>94</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.94</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.730</td>\n",
       "      <td>1.58</td>\n",
       "      <td>695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>3</td>\n",
       "      <td>12.25</td>\n",
       "      <td>4.72</td>\n",
       "      <td>2.54</td>\n",
       "      <td>21.0</td>\n",
       "      <td>89</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.80</td>\n",
       "      <td>3.850000</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.27</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>3</td>\n",
       "      <td>12.53</td>\n",
       "      <td>5.51</td>\n",
       "      <td>2.64</td>\n",
       "      <td>25.0</td>\n",
       "      <td>96</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.10</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.820</td>\n",
       "      <td>1.69</td>\n",
       "      <td>515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>3</td>\n",
       "      <td>13.49</td>\n",
       "      <td>3.59</td>\n",
       "      <td>2.19</td>\n",
       "      <td>19.5</td>\n",
       "      <td>88</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>0.810</td>\n",
       "      <td>1.82</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>3</td>\n",
       "      <td>12.84</td>\n",
       "      <td>2.96</td>\n",
       "      <td>2.61</td>\n",
       "      <td>24.0</td>\n",
       "      <td>101</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.81</td>\n",
       "      <td>4.920000</td>\n",
       "      <td>0.890</td>\n",
       "      <td>2.15</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>3</td>\n",
       "      <td>12.93</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2.70</td>\n",
       "      <td>21.0</td>\n",
       "      <td>96</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.75</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.770</td>\n",
       "      <td>2.31</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>3</td>\n",
       "      <td>13.36</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.35</td>\n",
       "      <td>20.0</td>\n",
       "      <td>89</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.64</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>2.47</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>3</td>\n",
       "      <td>13.52</td>\n",
       "      <td>3.17</td>\n",
       "      <td>2.72</td>\n",
       "      <td>23.5</td>\n",
       "      <td>97</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.55</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>0.890</td>\n",
       "      <td>2.06</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>3</td>\n",
       "      <td>13.62</td>\n",
       "      <td>4.95</td>\n",
       "      <td>2.35</td>\n",
       "      <td>20.0</td>\n",
       "      <td>92</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.02</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>0.910</td>\n",
       "      <td>2.05</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>3</td>\n",
       "      <td>12.25</td>\n",
       "      <td>3.88</td>\n",
       "      <td>2.20</td>\n",
       "      <td>18.5</td>\n",
       "      <td>112</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.14</td>\n",
       "      <td>8.210000</td>\n",
       "      <td>0.650</td>\n",
       "      <td>2.00</td>\n",
       "      <td>855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>3</td>\n",
       "      <td>13.16</td>\n",
       "      <td>3.57</td>\n",
       "      <td>2.15</td>\n",
       "      <td>21.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.30</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.600</td>\n",
       "      <td>1.68</td>\n",
       "      <td>830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>3</td>\n",
       "      <td>13.88</td>\n",
       "      <td>5.04</td>\n",
       "      <td>2.23</td>\n",
       "      <td>20.0</td>\n",
       "      <td>80</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.68</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>0.580</td>\n",
       "      <td>1.33</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>3</td>\n",
       "      <td>12.87</td>\n",
       "      <td>4.61</td>\n",
       "      <td>2.48</td>\n",
       "      <td>21.5</td>\n",
       "      <td>86</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.86</td>\n",
       "      <td>7.650000</td>\n",
       "      <td>0.540</td>\n",
       "      <td>1.86</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3</td>\n",
       "      <td>13.32</td>\n",
       "      <td>3.24</td>\n",
       "      <td>2.38</td>\n",
       "      <td>21.5</td>\n",
       "      <td>92</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.25</td>\n",
       "      <td>8.420000</td>\n",
       "      <td>0.550</td>\n",
       "      <td>1.62</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>3</td>\n",
       "      <td>13.08</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.36</td>\n",
       "      <td>21.5</td>\n",
       "      <td>113</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>0.570</td>\n",
       "      <td>1.33</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>3</td>\n",
       "      <td>13.50</td>\n",
       "      <td>3.12</td>\n",
       "      <td>2.62</td>\n",
       "      <td>24.0</td>\n",
       "      <td>123</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.25</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>0.590</td>\n",
       "      <td>1.30</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>3</td>\n",
       "      <td>12.79</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2.48</td>\n",
       "      <td>22.0</td>\n",
       "      <td>112</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.26</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>0.480</td>\n",
       "      <td>1.47</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>3</td>\n",
       "      <td>13.11</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.75</td>\n",
       "      <td>25.5</td>\n",
       "      <td>116</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.56</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.610</td>\n",
       "      <td>1.33</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>3</td>\n",
       "      <td>13.23</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2.28</td>\n",
       "      <td>18.5</td>\n",
       "      <td>98</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.87</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>0.560</td>\n",
       "      <td>1.51</td>\n",
       "      <td>675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>3</td>\n",
       "      <td>12.58</td>\n",
       "      <td>1.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>20.0</td>\n",
       "      <td>103</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.40</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>0.580</td>\n",
       "      <td>1.55</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>3</td>\n",
       "      <td>13.17</td>\n",
       "      <td>5.19</td>\n",
       "      <td>2.32</td>\n",
       "      <td>22.0</td>\n",
       "      <td>93</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.55</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.600</td>\n",
       "      <td>1.48</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>3</td>\n",
       "      <td>13.84</td>\n",
       "      <td>4.12</td>\n",
       "      <td>2.38</td>\n",
       "      <td>19.5</td>\n",
       "      <td>89</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.56</td>\n",
       "      <td>9.010000</td>\n",
       "      <td>0.570</td>\n",
       "      <td>1.64</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>3</td>\n",
       "      <td>12.45</td>\n",
       "      <td>3.03</td>\n",
       "      <td>2.64</td>\n",
       "      <td>27.0</td>\n",
       "      <td>97</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.14</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.670</td>\n",
       "      <td>1.73</td>\n",
       "      <td>880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>3</td>\n",
       "      <td>14.34</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.70</td>\n",
       "      <td>25.0</td>\n",
       "      <td>98</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2.70</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.570</td>\n",
       "      <td>1.96</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>3</td>\n",
       "      <td>13.48</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.64</td>\n",
       "      <td>22.5</td>\n",
       "      <td>89</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2.29</td>\n",
       "      <td>11.750000</td>\n",
       "      <td>0.570</td>\n",
       "      <td>1.78</td>\n",
       "      <td>620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>3</td>\n",
       "      <td>12.36</td>\n",
       "      <td>3.83</td>\n",
       "      <td>2.38</td>\n",
       "      <td>21.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.04</td>\n",
       "      <td>7.650000</td>\n",
       "      <td>0.560</td>\n",
       "      <td>1.58</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>3</td>\n",
       "      <td>13.69</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.54</td>\n",
       "      <td>20.0</td>\n",
       "      <td>107</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.80</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>0.960</td>\n",
       "      <td>1.82</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>3</td>\n",
       "      <td>12.85</td>\n",
       "      <td>3.27</td>\n",
       "      <td>2.58</td>\n",
       "      <td>22.0</td>\n",
       "      <td>106</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.96</td>\n",
       "      <td>5.580000</td>\n",
       "      <td>0.870</td>\n",
       "      <td>2.11</td>\n",
       "      <td>570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>3</td>\n",
       "      <td>12.96</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2.35</td>\n",
       "      <td>18.5</td>\n",
       "      <td>106</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.94</td>\n",
       "      <td>5.280000</td>\n",
       "      <td>0.680</td>\n",
       "      <td>1.75</td>\n",
       "      <td>675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>3</td>\n",
       "      <td>13.78</td>\n",
       "      <td>2.76</td>\n",
       "      <td>2.30</td>\n",
       "      <td>22.0</td>\n",
       "      <td>90</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.03</td>\n",
       "      <td>9.580000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1.68</td>\n",
       "      <td>615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>3</td>\n",
       "      <td>13.73</td>\n",
       "      <td>4.36</td>\n",
       "      <td>2.26</td>\n",
       "      <td>22.5</td>\n",
       "      <td>88</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.15</td>\n",
       "      <td>6.620000</td>\n",
       "      <td>0.780</td>\n",
       "      <td>1.75</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>3</td>\n",
       "      <td>13.45</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2.60</td>\n",
       "      <td>23.0</td>\n",
       "      <td>111</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.46</td>\n",
       "      <td>10.680000</td>\n",
       "      <td>0.850</td>\n",
       "      <td>1.56</td>\n",
       "      <td>695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>3</td>\n",
       "      <td>12.82</td>\n",
       "      <td>3.37</td>\n",
       "      <td>2.30</td>\n",
       "      <td>19.5</td>\n",
       "      <td>88</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.97</td>\n",
       "      <td>10.260000</td>\n",
       "      <td>0.720</td>\n",
       "      <td>1.75</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>3</td>\n",
       "      <td>13.58</td>\n",
       "      <td>2.58</td>\n",
       "      <td>2.69</td>\n",
       "      <td>24.5</td>\n",
       "      <td>105</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.54</td>\n",
       "      <td>8.660000</td>\n",
       "      <td>0.740</td>\n",
       "      <td>1.80</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>3</td>\n",
       "      <td>13.40</td>\n",
       "      <td>4.60</td>\n",
       "      <td>2.86</td>\n",
       "      <td>25.0</td>\n",
       "      <td>112</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.11</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>0.670</td>\n",
       "      <td>1.92</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>3</td>\n",
       "      <td>12.20</td>\n",
       "      <td>3.03</td>\n",
       "      <td>2.32</td>\n",
       "      <td>19.0</td>\n",
       "      <td>96</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.73</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>0.660</td>\n",
       "      <td>1.83</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>3</td>\n",
       "      <td>12.77</td>\n",
       "      <td>2.39</td>\n",
       "      <td>2.28</td>\n",
       "      <td>19.5</td>\n",
       "      <td>86</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.64</td>\n",
       "      <td>9.899999</td>\n",
       "      <td>0.570</td>\n",
       "      <td>1.63</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>3</td>\n",
       "      <td>14.16</td>\n",
       "      <td>2.51</td>\n",
       "      <td>2.48</td>\n",
       "      <td>20.0</td>\n",
       "      <td>91</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.24</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>0.620</td>\n",
       "      <td>1.71</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3</td>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>0.640</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3</td>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3</td>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>0.590</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3</td>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>0.600</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3</td>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.610</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Class  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0        1    14.23        1.71  2.43               15.6        127   \n",
       "1        1    13.20        1.78  2.14               11.2        100   \n",
       "2        1    13.16        2.36  2.67               18.6        101   \n",
       "3        1    14.37        1.95  2.50               16.8        113   \n",
       "4        1    13.24        2.59  2.87               21.0        118   \n",
       "5        1    14.20        1.76  2.45               15.2        112   \n",
       "6        1    14.39        1.87  2.45               14.6         96   \n",
       "7        1    14.06        2.15  2.61               17.6        121   \n",
       "8        1    14.83        1.64  2.17               14.0         97   \n",
       "9        1    13.86        1.35  2.27               16.0         98   \n",
       "10       1    14.10        2.16  2.30               18.0        105   \n",
       "11       1    14.12        1.48  2.32               16.8         95   \n",
       "12       1    13.75        1.73  2.41               16.0         89   \n",
       "13       1    14.75        1.73  2.39               11.4         91   \n",
       "14       1    14.38        1.87  2.38               12.0        102   \n",
       "15       1    13.63        1.81  2.70               17.2        112   \n",
       "16       1    14.30        1.92  2.72               20.0        120   \n",
       "17       1    13.83        1.57  2.62               20.0        115   \n",
       "18       1    14.19        1.59  2.48               16.5        108   \n",
       "19       1    13.64        3.10  2.56               15.2        116   \n",
       "20       1    14.06        1.63  2.28               16.0        126   \n",
       "21       1    12.93        3.80  2.65               18.6        102   \n",
       "22       1    13.71        1.86  2.36               16.6        101   \n",
       "23       1    12.85        1.60  2.52               17.8         95   \n",
       "24       1    13.50        1.81  2.61               20.0         96   \n",
       "25       1    13.05        2.05  3.22               25.0        124   \n",
       "26       1    13.39        1.77  2.62               16.1         93   \n",
       "27       1    13.30        1.72  2.14               17.0         94   \n",
       "28       1    13.87        1.90  2.80               19.4        107   \n",
       "29       1    14.02        1.68  2.21               16.0         96   \n",
       "30       1    13.73        1.50  2.70               22.5        101   \n",
       "31       1    13.58        1.66  2.36               19.1        106   \n",
       "32       1    13.68        1.83  2.36               17.2        104   \n",
       "33       1    13.76        1.53  2.70               19.5        132   \n",
       "34       1    13.51        1.80  2.65               19.0        110   \n",
       "35       1    13.48        1.81  2.41               20.5        100   \n",
       "36       1    13.28        1.64  2.84               15.5        110   \n",
       "37       1    13.05        1.65  2.55               18.0         98   \n",
       "38       1    13.07        1.50  2.10               15.5         98   \n",
       "39       1    14.22        3.99  2.51               13.2        128   \n",
       "40       1    13.56        1.71  2.31               16.2        117   \n",
       "41       1    13.41        3.84  2.12               18.8         90   \n",
       "42       1    13.88        1.89  2.59               15.0        101   \n",
       "43       1    13.24        3.98  2.29               17.5        103   \n",
       "44       1    13.05        1.77  2.10               17.0        107   \n",
       "45       1    14.21        4.04  2.44               18.9        111   \n",
       "46       1    14.38        3.59  2.28               16.0        102   \n",
       "47       1    13.90        1.68  2.12               16.0        101   \n",
       "48       1    14.10        2.02  2.40               18.8        103   \n",
       "49       1    13.94        1.73  2.27               17.4        108   \n",
       "50       1    13.05        1.73  2.04               12.4         92   \n",
       "51       1    13.83        1.65  2.60               17.2         94   \n",
       "52       1    13.82        1.75  2.42               14.0        111   \n",
       "53       1    13.77        1.90  2.68               17.1        115   \n",
       "54       1    13.74        1.67  2.25               16.4        118   \n",
       "55       1    13.56        1.73  2.46               20.5        116   \n",
       "56       1    14.22        1.70  2.30               16.3        118   \n",
       "57       1    13.29        1.97  2.68               16.8        102   \n",
       "58       1    13.72        1.43  2.50               16.7        108   \n",
       "59       2    12.37        0.94  1.36               10.6         88   \n",
       "60       2    12.33        1.10  2.28               16.0        101   \n",
       "61       2    12.64        1.36  2.02               16.8        100   \n",
       "62       2    13.67        1.25  1.92               18.0         94   \n",
       "63       2    12.37        1.13  2.16               19.0         87   \n",
       "64       2    12.17        1.45  2.53               19.0        104   \n",
       "65       2    12.37        1.21  2.56               18.1         98   \n",
       "66       2    13.11        1.01  1.70               15.0         78   \n",
       "67       2    12.37        1.17  1.92               19.6         78   \n",
       "68       2    13.34        0.94  2.36               17.0        110   \n",
       "69       2    12.21        1.19  1.75               16.8        151   \n",
       "70       2    12.29        1.61  2.21               20.4        103   \n",
       "71       2    13.86        1.51  2.67               25.0         86   \n",
       "72       2    13.49        1.66  2.24               24.0         87   \n",
       "73       2    12.99        1.67  2.60               30.0        139   \n",
       "74       2    11.96        1.09  2.30               21.0        101   \n",
       "75       2    11.66        1.88  1.92               16.0         97   \n",
       "76       2    13.03        0.90  1.71               16.0         86   \n",
       "77       2    11.84        2.89  2.23               18.0        112   \n",
       "78       2    12.33        0.99  1.95               14.8        136   \n",
       "79       2    12.70        3.87  2.40               23.0        101   \n",
       "80       2    12.00        0.92  2.00               19.0         86   \n",
       "81       2    12.72        1.81  2.20               18.8         86   \n",
       "82       2    12.08        1.13  2.51               24.0         78   \n",
       "83       2    13.05        3.86  2.32               22.5         85   \n",
       "84       2    11.84        0.89  2.58               18.0         94   \n",
       "85       2    12.67        0.98  2.24               18.0         99   \n",
       "86       2    12.16        1.61  2.31               22.8         90   \n",
       "87       2    11.65        1.67  2.62               26.0         88   \n",
       "88       2    11.64        2.06  2.46               21.6         84   \n",
       "89       2    12.08        1.33  2.30               23.6         70   \n",
       "90       2    12.08        1.83  2.32               18.5         81   \n",
       "91       2    12.00        1.51  2.42               22.0         86   \n",
       "92       2    12.69        1.53  2.26               20.7         80   \n",
       "93       2    12.29        2.83  2.22               18.0         88   \n",
       "94       2    11.62        1.99  2.28               18.0         98   \n",
       "95       2    12.47        1.52  2.20               19.0        162   \n",
       "96       2    11.81        2.12  2.74               21.5        134   \n",
       "97       2    12.29        1.41  1.98               16.0         85   \n",
       "98       2    12.37        1.07  2.10               18.5         88   \n",
       "99       2    12.29        3.17  2.21               18.0         88   \n",
       "100      2    12.08        2.08  1.70               17.5         97   \n",
       "101      2    12.60        1.34  1.90               18.5         88   \n",
       "102      2    12.34        2.45  2.46               21.0         98   \n",
       "103      2    11.82        1.72  1.88               19.5         86   \n",
       "104      2    12.51        1.73  1.98               20.5         85   \n",
       "105      2    12.42        2.55  2.27               22.0         90   \n",
       "106      2    12.25        1.73  2.12               19.0         80   \n",
       "107      2    12.72        1.75  2.28               22.5         84   \n",
       "108      2    12.22        1.29  1.94               19.0         92   \n",
       "109      2    11.61        1.35  2.70               20.0         94   \n",
       "110      2    11.46        3.74  1.82               19.5        107   \n",
       "111      2    12.52        2.43  2.17               21.0         88   \n",
       "112      2    11.76        2.68  2.92               20.0        103   \n",
       "113      2    11.41        0.74  2.50               21.0         88   \n",
       "114      2    12.08        1.39  2.50               22.5         84   \n",
       "115      2    11.03        1.51  2.20               21.5         85   \n",
       "116      2    11.82        1.47  1.99               20.8         86   \n",
       "117      2    12.42        1.61  2.19               22.5        108   \n",
       "118      2    12.77        3.43  1.98               16.0         80   \n",
       "119      2    12.00        3.43  2.00               19.0         87   \n",
       "120      2    11.45        2.40  2.42               20.0         96   \n",
       "121      2    11.56        2.05  3.23               28.5        119   \n",
       "122      2    12.42        4.43  2.73               26.5        102   \n",
       "123      2    13.05        5.80  2.13               21.5         86   \n",
       "124      2    11.87        4.31  2.39               21.0         82   \n",
       "125      2    12.07        2.16  2.17               21.0         85   \n",
       "126      2    12.43        1.53  2.29               21.5         86   \n",
       "127      2    11.79        2.13  2.78               28.5         92   \n",
       "128      2    12.37        1.63  2.30               24.5         88   \n",
       "129      2    12.04        4.30  2.38               22.0         80   \n",
       "130      3    12.86        1.35  2.32               18.0        122   \n",
       "131      3    12.88        2.99  2.40               20.0        104   \n",
       "132      3    12.81        2.31  2.40               24.0         98   \n",
       "133      3    12.70        3.55  2.36               21.5        106   \n",
       "134      3    12.51        1.24  2.25               17.5         85   \n",
       "135      3    12.60        2.46  2.20               18.5         94   \n",
       "136      3    12.25        4.72  2.54               21.0         89   \n",
       "137      3    12.53        5.51  2.64               25.0         96   \n",
       "138      3    13.49        3.59  2.19               19.5         88   \n",
       "139      3    12.84        2.96  2.61               24.0        101   \n",
       "140      3    12.93        2.81  2.70               21.0         96   \n",
       "141      3    13.36        2.56  2.35               20.0         89   \n",
       "142      3    13.52        3.17  2.72               23.5         97   \n",
       "143      3    13.62        4.95  2.35               20.0         92   \n",
       "144      3    12.25        3.88  2.20               18.5        112   \n",
       "145      3    13.16        3.57  2.15               21.0        102   \n",
       "146      3    13.88        5.04  2.23               20.0         80   \n",
       "147      3    12.87        4.61  2.48               21.5         86   \n",
       "148      3    13.32        3.24  2.38               21.5         92   \n",
       "149      3    13.08        3.90  2.36               21.5        113   \n",
       "150      3    13.50        3.12  2.62               24.0        123   \n",
       "151      3    12.79        2.67  2.48               22.0        112   \n",
       "152      3    13.11        1.90  2.75               25.5        116   \n",
       "153      3    13.23        3.30  2.28               18.5         98   \n",
       "154      3    12.58        1.29  2.10               20.0        103   \n",
       "155      3    13.17        5.19  2.32               22.0         93   \n",
       "156      3    13.84        4.12  2.38               19.5         89   \n",
       "157      3    12.45        3.03  2.64               27.0         97   \n",
       "158      3    14.34        1.68  2.70               25.0         98   \n",
       "159      3    13.48        1.67  2.64               22.5         89   \n",
       "160      3    12.36        3.83  2.38               21.0         88   \n",
       "161      3    13.69        3.26  2.54               20.0        107   \n",
       "162      3    12.85        3.27  2.58               22.0        106   \n",
       "163      3    12.96        3.45  2.35               18.5        106   \n",
       "164      3    13.78        2.76  2.30               22.0         90   \n",
       "165      3    13.73        4.36  2.26               22.5         88   \n",
       "166      3    13.45        3.70  2.60               23.0        111   \n",
       "167      3    12.82        3.37  2.30               19.5         88   \n",
       "168      3    13.58        2.58  2.69               24.5        105   \n",
       "169      3    13.40        4.60  2.86               25.0        112   \n",
       "170      3    12.20        3.03  2.32               19.0         96   \n",
       "171      3    12.77        2.39  2.28               19.5         86   \n",
       "172      3    14.16        2.51  2.48               20.0         91   \n",
       "173      3    13.71        5.65  2.45               20.5         95   \n",
       "174      3    13.40        3.91  2.48               23.0        102   \n",
       "175      3    13.27        4.28  2.26               20.0        120   \n",
       "176      3    13.17        2.59  2.37               20.0        120   \n",
       "177      3    14.13        4.10  2.74               24.5         96   \n",
       "\n",
       "     Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0             2.80        3.06                  0.28             2.29   \n",
       "1             2.65        2.76                  0.26             1.28   \n",
       "2             2.80        3.24                  0.30             2.81   \n",
       "3             3.85        3.49                  0.24             2.18   \n",
       "4             2.80        2.69                  0.39             1.82   \n",
       "5             3.27        3.39                  0.34             1.97   \n",
       "6             2.50        2.52                  0.30             1.98   \n",
       "7             2.60        2.51                  0.31             1.25   \n",
       "8             2.80        2.98                  0.29             1.98   \n",
       "9             2.98        3.15                  0.22             1.85   \n",
       "10            2.95        3.32                  0.22             2.38   \n",
       "11            2.20        2.43                  0.26             1.57   \n",
       "12            2.60        2.76                  0.29             1.81   \n",
       "13            3.10        3.69                  0.43             2.81   \n",
       "14            3.30        3.64                  0.29             2.96   \n",
       "15            2.85        2.91                  0.30             1.46   \n",
       "16            2.80        3.14                  0.33             1.97   \n",
       "17            2.95        3.40                  0.40             1.72   \n",
       "18            3.30        3.93                  0.32             1.86   \n",
       "19            2.70        3.03                  0.17             1.66   \n",
       "20            3.00        3.17                  0.24             2.10   \n",
       "21            2.41        2.41                  0.25             1.98   \n",
       "22            2.61        2.88                  0.27             1.69   \n",
       "23            2.48        2.37                  0.26             1.46   \n",
       "24            2.53        2.61                  0.28             1.66   \n",
       "25            2.63        2.68                  0.47             1.92   \n",
       "26            2.85        2.94                  0.34             1.45   \n",
       "27            2.40        2.19                  0.27             1.35   \n",
       "28            2.95        2.97                  0.37             1.76   \n",
       "29            2.65        2.33                  0.26             1.98   \n",
       "30            3.00        3.25                  0.29             2.38   \n",
       "31            2.86        3.19                  0.22             1.95   \n",
       "32            2.42        2.69                  0.42             1.97   \n",
       "33            2.95        2.74                  0.50             1.35   \n",
       "34            2.35        2.53                  0.29             1.54   \n",
       "35            2.70        2.98                  0.26             1.86   \n",
       "36            2.60        2.68                  0.34             1.36   \n",
       "37            2.45        2.43                  0.29             1.44   \n",
       "38            2.40        2.64                  0.28             1.37   \n",
       "39            3.00        3.04                  0.20             2.08   \n",
       "40            3.15        3.29                  0.34             2.34   \n",
       "41            2.45        2.68                  0.27             1.48   \n",
       "42            3.25        3.56                  0.17             1.70   \n",
       "43            2.64        2.63                  0.32             1.66   \n",
       "44            3.00        3.00                  0.28             2.03   \n",
       "45            2.85        2.65                  0.30             1.25   \n",
       "46            3.25        3.17                  0.27             2.19   \n",
       "47            3.10        3.39                  0.21             2.14   \n",
       "48            2.75        2.92                  0.32             2.38   \n",
       "49            2.88        3.54                  0.32             2.08   \n",
       "50            2.72        3.27                  0.17             2.91   \n",
       "51            2.45        2.99                  0.22             2.29   \n",
       "52            3.88        3.74                  0.32             1.87   \n",
       "53            3.00        2.79                  0.39             1.68   \n",
       "54            2.60        2.90                  0.21             1.62   \n",
       "55            2.96        2.78                  0.20             2.45   \n",
       "56            3.20        3.00                  0.26             2.03   \n",
       "57            3.00        3.23                  0.31             1.66   \n",
       "58            3.40        3.67                  0.19             2.04   \n",
       "59            1.98        0.57                  0.28             0.42   \n",
       "60            2.05        1.09                  0.63             0.41   \n",
       "61            2.02        1.41                  0.53             0.62   \n",
       "62            2.10        1.79                  0.32             0.73   \n",
       "63            3.50        3.10                  0.19             1.87   \n",
       "64            1.89        1.75                  0.45             1.03   \n",
       "65            2.42        2.65                  0.37             2.08   \n",
       "66            2.98        3.18                  0.26             2.28   \n",
       "67            2.11        2.00                  0.27             1.04   \n",
       "68            2.53        1.30                  0.55             0.42   \n",
       "69            1.85        1.28                  0.14             2.50   \n",
       "70            1.10        1.02                  0.37             1.46   \n",
       "71            2.95        2.86                  0.21             1.87   \n",
       "72            1.88        1.84                  0.27             1.03   \n",
       "73            3.30        2.89                  0.21             1.96   \n",
       "74            3.38        2.14                  0.13             1.65   \n",
       "75            1.61        1.57                  0.34             1.15   \n",
       "76            1.95        2.03                  0.24             1.46   \n",
       "77            1.72        1.32                  0.43             0.95   \n",
       "78            1.90        1.85                  0.35             2.76   \n",
       "79            2.83        2.55                  0.43             1.95   \n",
       "80            2.42        2.26                  0.30             1.43   \n",
       "81            2.20        2.53                  0.26             1.77   \n",
       "82            2.00        1.58                  0.40             1.40   \n",
       "83            1.65        1.59                  0.61             1.62   \n",
       "84            2.20        2.21                  0.22             2.35   \n",
       "85            2.20        1.94                  0.30             1.46   \n",
       "86            1.78        1.69                  0.43             1.56   \n",
       "87            1.92        1.61                  0.40             1.34   \n",
       "88            1.95        1.69                  0.48             1.35   \n",
       "89            2.20        1.59                  0.42             1.38   \n",
       "90            1.60        1.50                  0.52             1.64   \n",
       "91            1.45        1.25                  0.50             1.63   \n",
       "92            1.38        1.46                  0.58             1.62   \n",
       "93            2.45        2.25                  0.25             1.99   \n",
       "94            3.02        2.26                  0.17             1.35   \n",
       "95            2.50        2.27                  0.32             3.28   \n",
       "96            1.60        0.99                  0.14             1.56   \n",
       "97            2.55        2.50                  0.29             1.77   \n",
       "98            3.52        3.75                  0.24             1.95   \n",
       "99            2.85        2.99                  0.45             2.81   \n",
       "100           2.23        2.17                  0.26             1.40   \n",
       "101           1.45        1.36                  0.29             1.35   \n",
       "102           2.56        2.11                  0.34             1.31   \n",
       "103           2.50        1.64                  0.37             1.42   \n",
       "104           2.20        1.92                  0.32             1.48   \n",
       "105           1.68        1.84                  0.66             1.42   \n",
       "106           1.65        2.03                  0.37             1.63   \n",
       "107           1.38        1.76                  0.48             1.63   \n",
       "108           2.36        2.04                  0.39             2.08   \n",
       "109           2.74        2.92                  0.29             2.49   \n",
       "110           3.18        2.58                  0.24             3.58   \n",
       "111           2.55        2.27                  0.26             1.22   \n",
       "112           1.75        2.03                  0.60             1.05   \n",
       "113           2.48        2.01                  0.42             1.44   \n",
       "114           2.56        2.29                  0.43             1.04   \n",
       "115           2.46        2.17                  0.52             2.01   \n",
       "116           1.98        1.60                  0.30             1.53   \n",
       "117           2.00        2.09                  0.34             1.61   \n",
       "118           1.63        1.25                  0.43             0.83   \n",
       "119           2.00        1.64                  0.37             1.87   \n",
       "120           2.90        2.79                  0.32             1.83   \n",
       "121           3.18        5.08                  0.47             1.87   \n",
       "122           2.20        2.13                  0.43             1.71   \n",
       "123           2.62        2.65                  0.30             2.01   \n",
       "124           2.86        3.03                  0.21             2.91   \n",
       "125           2.60        2.65                  0.37             1.35   \n",
       "126           2.74        3.15                  0.39             1.77   \n",
       "127           2.13        2.24                  0.58             1.76   \n",
       "128           2.22        2.45                  0.40             1.90   \n",
       "129           2.10        1.75                  0.42             1.35   \n",
       "130           1.51        1.25                  0.21             0.94   \n",
       "131           1.30        1.22                  0.24             0.83   \n",
       "132           1.15        1.09                  0.27             0.83   \n",
       "133           1.70        1.20                  0.17             0.84   \n",
       "134           2.00        0.58                  0.60             1.25   \n",
       "135           1.62        0.66                  0.63             0.94   \n",
       "136           1.38        0.47                  0.53             0.80   \n",
       "137           1.79        0.60                  0.63             1.10   \n",
       "138           1.62        0.48                  0.58             0.88   \n",
       "139           2.32        0.60                  0.53             0.81   \n",
       "140           1.54        0.50                  0.53             0.75   \n",
       "141           1.40        0.50                  0.37             0.64   \n",
       "142           1.55        0.52                  0.50             0.55   \n",
       "143           2.00        0.80                  0.47             1.02   \n",
       "144           1.38        0.78                  0.29             1.14   \n",
       "145           1.50        0.55                  0.43             1.30   \n",
       "146           0.98        0.34                  0.40             0.68   \n",
       "147           1.70        0.65                  0.47             0.86   \n",
       "148           1.93        0.76                  0.45             1.25   \n",
       "149           1.41        1.39                  0.34             1.14   \n",
       "150           1.40        1.57                  0.22             1.25   \n",
       "151           1.48        1.36                  0.24             1.26   \n",
       "152           2.20        1.28                  0.26             1.56   \n",
       "153           1.80        0.83                  0.61             1.87   \n",
       "154           1.48        0.58                  0.53             1.40   \n",
       "155           1.74        0.63                  0.61             1.55   \n",
       "156           1.80        0.83                  0.48             1.56   \n",
       "157           1.90        0.58                  0.63             1.14   \n",
       "158           2.80        1.31                  0.53             2.70   \n",
       "159           2.60        1.10                  0.52             2.29   \n",
       "160           2.30        0.92                  0.50             1.04   \n",
       "161           1.83        0.56                  0.50             0.80   \n",
       "162           1.65        0.60                  0.60             0.96   \n",
       "163           1.39        0.70                  0.40             0.94   \n",
       "164           1.35        0.68                  0.41             1.03   \n",
       "165           1.28        0.47                  0.52             1.15   \n",
       "166           1.70        0.92                  0.43             1.46   \n",
       "167           1.48        0.66                  0.40             0.97   \n",
       "168           1.55        0.84                  0.39             1.54   \n",
       "169           1.98        0.96                  0.27             1.11   \n",
       "170           1.25        0.49                  0.40             0.73   \n",
       "171           1.39        0.51                  0.48             0.64   \n",
       "172           1.68        0.70                  0.44             1.24   \n",
       "173           1.68        0.61                  0.52             1.06   \n",
       "174           1.80        0.75                  0.43             1.41   \n",
       "175           1.59        0.69                  0.43             1.35   \n",
       "176           1.65        0.68                  0.53             1.46   \n",
       "177           2.05        0.76                  0.56             1.35   \n",
       "\n",
       "     Color intensity    Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0           5.640000  1.040                          3.92     1065  \n",
       "1           4.380000  1.050                          3.40     1050  \n",
       "2           5.680000  1.030                          3.17     1185  \n",
       "3           7.800000  0.860                          3.45     1480  \n",
       "4           4.320000  1.040                          2.93      735  \n",
       "5           6.750000  1.050                          2.85     1450  \n",
       "6           5.250000  1.020                          3.58     1290  \n",
       "7           5.050000  1.060                          3.58     1295  \n",
       "8           5.200000  1.080                          2.85     1045  \n",
       "9           7.220000  1.010                          3.55     1045  \n",
       "10          5.750000  1.250                          3.17     1510  \n",
       "11          5.000000  1.170                          2.82     1280  \n",
       "12          5.600000  1.150                          2.90     1320  \n",
       "13          5.400000  1.250                          2.73     1150  \n",
       "14          7.500000  1.200                          3.00     1547  \n",
       "15          7.300000  1.280                          2.88     1310  \n",
       "16          6.200000  1.070                          2.65     1280  \n",
       "17          6.600000  1.130                          2.57     1130  \n",
       "18          8.700000  1.230                          2.82     1680  \n",
       "19          5.100000  0.960                          3.36      845  \n",
       "20          5.650000  1.090                          3.71      780  \n",
       "21          4.500000  1.030                          3.52      770  \n",
       "22          3.800000  1.110                          4.00     1035  \n",
       "23          3.930000  1.090                          3.63     1015  \n",
       "24          3.520000  1.120                          3.82      845  \n",
       "25          3.580000  1.130                          3.20      830  \n",
       "26          4.800000  0.920                          3.22     1195  \n",
       "27          3.950000  1.020                          2.77     1285  \n",
       "28          4.500000  1.250                          3.40      915  \n",
       "29          4.700000  1.040                          3.59     1035  \n",
       "30          5.700000  1.190                          2.71     1285  \n",
       "31          6.900000  1.090                          2.88     1515  \n",
       "32          3.840000  1.230                          2.87      990  \n",
       "33          5.400000  1.250                          3.00     1235  \n",
       "34          4.200000  1.100                          2.87     1095  \n",
       "35          5.100000  1.040                          3.47      920  \n",
       "36          4.600000  1.090                          2.78      880  \n",
       "37          4.250000  1.120                          2.51     1105  \n",
       "38          3.700000  1.180                          2.69     1020  \n",
       "39          5.100000  0.890                          3.53      760  \n",
       "40          6.130000  0.950                          3.38      795  \n",
       "41          4.280000  0.910                          3.00     1035  \n",
       "42          5.430000  0.880                          3.56     1095  \n",
       "43          4.360000  0.820                          3.00      680  \n",
       "44          5.040000  0.880                          3.35      885  \n",
       "45          5.240000  0.870                          3.33     1080  \n",
       "46          4.900000  1.040                          3.44     1065  \n",
       "47          6.100000  0.910                          3.33      985  \n",
       "48          6.200000  1.070                          2.75     1060  \n",
       "49          8.900000  1.120                          3.10     1260  \n",
       "50          7.200000  1.120                          2.91     1150  \n",
       "51          5.600000  1.240                          3.37     1265  \n",
       "52          7.050000  1.010                          3.26     1190  \n",
       "53          6.300000  1.130                          2.93     1375  \n",
       "54          5.850000  0.920                          3.20     1060  \n",
       "55          6.250000  0.980                          3.03     1120  \n",
       "56          6.380000  0.940                          3.31      970  \n",
       "57          6.000000  1.070                          2.84     1270  \n",
       "58          6.800000  0.890                          2.87     1285  \n",
       "59          1.950000  1.050                          1.82      520  \n",
       "60          3.270000  1.250                          1.67      680  \n",
       "61          5.750000  0.980                          1.59      450  \n",
       "62          3.800000  1.230                          2.46      630  \n",
       "63          4.450000  1.220                          2.87      420  \n",
       "64          2.950000  1.450                          2.23      355  \n",
       "65          4.600000  1.190                          2.30      678  \n",
       "66          5.300000  1.120                          3.18      502  \n",
       "67          4.680000  1.120                          3.48      510  \n",
       "68          3.170000  1.020                          1.93      750  \n",
       "69          2.850000  1.280                          3.07      718  \n",
       "70          3.050000  0.906                          1.82      870  \n",
       "71          3.380000  1.360                          3.16      410  \n",
       "72          3.740000  0.980                          2.78      472  \n",
       "73          3.350000  1.310                          3.50      985  \n",
       "74          3.210000  0.990                          3.13      886  \n",
       "75          3.800000  1.230                          2.14      428  \n",
       "76          4.600000  1.190                          2.48      392  \n",
       "77          2.650000  0.960                          2.52      500  \n",
       "78          3.400000  1.060                          2.31      750  \n",
       "79          2.570000  1.190                          3.13      463  \n",
       "80          2.500000  1.380                          3.12      278  \n",
       "81          3.900000  1.160                          3.14      714  \n",
       "82          2.200000  1.310                          2.72      630  \n",
       "83          4.800000  0.840                          2.01      515  \n",
       "84          3.050000  0.790                          3.08      520  \n",
       "85          2.620000  1.230                          3.16      450  \n",
       "86          2.450000  1.330                          2.26      495  \n",
       "87          2.600000  1.360                          3.21      562  \n",
       "88          2.800000  1.000                          2.75      680  \n",
       "89          1.740000  1.070                          3.21      625  \n",
       "90          2.400000  1.080                          2.27      480  \n",
       "91          3.600000  1.050                          2.65      450  \n",
       "92          3.050000  0.960                          2.06      495  \n",
       "93          2.150000  1.150                          3.30      290  \n",
       "94          3.250000  1.160                          2.96      345  \n",
       "95          2.600000  1.160                          2.63      937  \n",
       "96          2.500000  0.950                          2.26      625  \n",
       "97          2.900000  1.230                          2.74      428  \n",
       "98          4.500000  1.040                          2.77      660  \n",
       "99          2.300000  1.420                          2.83      406  \n",
       "100         3.300000  1.270                          2.96      710  \n",
       "101         2.450000  1.040                          2.77      562  \n",
       "102         2.800000  0.800                          3.38      438  \n",
       "103         2.060000  0.940                          2.44      415  \n",
       "104         2.940000  1.040                          3.57      672  \n",
       "105         2.700000  0.860                          3.30      315  \n",
       "106         3.400000  1.000                          3.17      510  \n",
       "107         3.300000  0.880                          2.42      488  \n",
       "108         2.700000  0.860                          3.02      312  \n",
       "109         2.650000  0.960                          3.26      680  \n",
       "110         2.900000  0.750                          2.81      562  \n",
       "111         2.000000  0.900                          2.78      325  \n",
       "112         3.800000  1.230                          2.50      607  \n",
       "113         3.080000  1.100                          2.31      434  \n",
       "114         2.900000  0.930                          3.19      385  \n",
       "115         1.900000  1.710                          2.87      407  \n",
       "116         1.950000  0.950                          3.33      495  \n",
       "117         2.060000  1.060                          2.96      345  \n",
       "118         3.400000  0.700                          2.12      372  \n",
       "119         1.280000  0.930                          3.05      564  \n",
       "120         3.250000  0.800                          3.39      625  \n",
       "121         6.000000  0.930                          3.69      465  \n",
       "122         2.080000  0.920                          3.12      365  \n",
       "123         2.600000  0.730                          3.10      380  \n",
       "124         2.800000  0.750                          3.64      380  \n",
       "125         2.760000  0.860                          3.28      378  \n",
       "126         3.940000  0.690                          2.84      352  \n",
       "127         3.000000  0.970                          2.44      466  \n",
       "128         2.120000  0.890                          2.78      342  \n",
       "129         2.600000  0.790                          2.57      580  \n",
       "130         4.100000  0.760                          1.29      630  \n",
       "131         5.400000  0.740                          1.42      530  \n",
       "132         5.700000  0.660                          1.36      560  \n",
       "133         5.000000  0.780                          1.29      600  \n",
       "134         5.450000  0.750                          1.51      650  \n",
       "135         7.100000  0.730                          1.58      695  \n",
       "136         3.850000  0.750                          1.27      720  \n",
       "137         5.000000  0.820                          1.69      515  \n",
       "138         5.700000  0.810                          1.82      580  \n",
       "139         4.920000  0.890                          2.15      590  \n",
       "140         4.600000  0.770                          2.31      600  \n",
       "141         5.600000  0.700                          2.47      780  \n",
       "142         4.350000  0.890                          2.06      520  \n",
       "143         4.400000  0.910                          2.05      550  \n",
       "144         8.210000  0.650                          2.00      855  \n",
       "145         4.000000  0.600                          1.68      830  \n",
       "146         4.900000  0.580                          1.33      415  \n",
       "147         7.650000  0.540                          1.86      625  \n",
       "148         8.420000  0.550                          1.62      650  \n",
       "149         9.400000  0.570                          1.33      550  \n",
       "150         8.600000  0.590                          1.30      500  \n",
       "151        10.800000  0.480                          1.47      480  \n",
       "152         7.100000  0.610                          1.33      425  \n",
       "153        10.520000  0.560                          1.51      675  \n",
       "154         7.600000  0.580                          1.55      640  \n",
       "155         7.900000  0.600                          1.48      725  \n",
       "156         9.010000  0.570                          1.64      480  \n",
       "157         7.500000  0.670                          1.73      880  \n",
       "158        13.000000  0.570                          1.96      660  \n",
       "159        11.750000  0.570                          1.78      620  \n",
       "160         7.650000  0.560                          1.58      520  \n",
       "161         5.880000  0.960                          1.82      680  \n",
       "162         5.580000  0.870                          2.11      570  \n",
       "163         5.280000  0.680                          1.75      675  \n",
       "164         9.580000  0.700                          1.68      615  \n",
       "165         6.620000  0.780                          1.75      520  \n",
       "166        10.680000  0.850                          1.56      695  \n",
       "167        10.260000  0.720                          1.75      685  \n",
       "168         8.660000  0.740                          1.80      750  \n",
       "169         8.500000  0.670                          1.92      630  \n",
       "170         5.500000  0.660                          1.83      510  \n",
       "171         9.899999  0.570                          1.63      470  \n",
       "172         9.700000  0.620                          1.71      660  \n",
       "173         7.700000  0.640                          1.74      740  \n",
       "174         7.300000  0.700                          1.56      750  \n",
       "175        10.200000  0.590                          1.56      835  \n",
       "176         9.300000  0.600                          1.62      840  \n",
       "177         9.200000  0.610                          1.60      560  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carreguem les dades a analitzar\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.read_csv('wineData.txt')\n",
    "\n",
    "# Afegim el nom de les columnes (constityents del vi) i afegim la columna 'Class' (cada un del cultius)\n",
    "\n",
    "columnes = ['Class', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids',\n",
    "            'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines','Proline']\n",
    "\n",
    "wine_df = pd.read_csv('wineData.txt', names = columnes)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "wine_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b206e5d",
   "metadata": {},
   "source": [
    "- Anàlisi previ del DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c22cbdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.938202</td>\n",
       "      <td>13.000618</td>\n",
       "      <td>2.336348</td>\n",
       "      <td>2.366517</td>\n",
       "      <td>19.494944</td>\n",
       "      <td>99.741573</td>\n",
       "      <td>2.295112</td>\n",
       "      <td>2.029270</td>\n",
       "      <td>0.361854</td>\n",
       "      <td>1.590899</td>\n",
       "      <td>5.058090</td>\n",
       "      <td>0.957449</td>\n",
       "      <td>2.611685</td>\n",
       "      <td>746.893258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.775035</td>\n",
       "      <td>0.811827</td>\n",
       "      <td>1.117146</td>\n",
       "      <td>0.274344</td>\n",
       "      <td>3.339564</td>\n",
       "      <td>14.282484</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.998859</td>\n",
       "      <td>0.124453</td>\n",
       "      <td>0.572359</td>\n",
       "      <td>2.318286</td>\n",
       "      <td>0.228572</td>\n",
       "      <td>0.709990</td>\n",
       "      <td>314.907474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.030000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>278.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.362500</td>\n",
       "      <td>1.602500</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>1.742500</td>\n",
       "      <td>1.205000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>3.220000</td>\n",
       "      <td>0.782500</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>500.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>13.050000</td>\n",
       "      <td>1.865000</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>2.355000</td>\n",
       "      <td>2.135000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.555000</td>\n",
       "      <td>4.690000</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>673.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>13.677500</td>\n",
       "      <td>3.082500</td>\n",
       "      <td>2.557500</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>3.170000</td>\n",
       "      <td>985.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>14.830000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.230000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>3.880000</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>3.580000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.710000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1680.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Class     Alcohol  Malic acid         Ash  Alcalinity of ash  \\\n",
       "count  178.000000  178.000000  178.000000  178.000000         178.000000   \n",
       "mean     1.938202   13.000618    2.336348    2.366517          19.494944   \n",
       "std      0.775035    0.811827    1.117146    0.274344           3.339564   \n",
       "min      1.000000   11.030000    0.740000    1.360000          10.600000   \n",
       "25%      1.000000   12.362500    1.602500    2.210000          17.200000   \n",
       "50%      2.000000   13.050000    1.865000    2.360000          19.500000   \n",
       "75%      3.000000   13.677500    3.082500    2.557500          21.500000   \n",
       "max      3.000000   14.830000    5.800000    3.230000          30.000000   \n",
       "\n",
       "        Magnesium  Total phenols  Flavanoids  Nonflavanoid phenols  \\\n",
       "count  178.000000     178.000000  178.000000            178.000000   \n",
       "mean    99.741573       2.295112    2.029270              0.361854   \n",
       "std     14.282484       0.625851    0.998859              0.124453   \n",
       "min     70.000000       0.980000    0.340000              0.130000   \n",
       "25%     88.000000       1.742500    1.205000              0.270000   \n",
       "50%     98.000000       2.355000    2.135000              0.340000   \n",
       "75%    107.000000       2.800000    2.875000              0.437500   \n",
       "max    162.000000       3.880000    5.080000              0.660000   \n",
       "\n",
       "       Proanthocyanins  Color intensity         Hue  \\\n",
       "count       178.000000       178.000000  178.000000   \n",
       "mean          1.590899         5.058090    0.957449   \n",
       "std           0.572359         2.318286    0.228572   \n",
       "min           0.410000         1.280000    0.480000   \n",
       "25%           1.250000         3.220000    0.782500   \n",
       "50%           1.555000         4.690000    0.965000   \n",
       "75%           1.950000         6.200000    1.120000   \n",
       "max           3.580000        13.000000    1.710000   \n",
       "\n",
       "       OD280/OD315 of diluted wines      Proline  \n",
       "count                    178.000000   178.000000  \n",
       "mean                       2.611685   746.893258  \n",
       "std                        0.709990   314.907474  \n",
       "min                        1.270000   278.000000  \n",
       "25%                        1.937500   500.500000  \n",
       "50%                        2.780000   673.500000  \n",
       "75%                        3.170000   985.000000  \n",
       "max                        4.000000  1680.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "560e1dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 178 entries, 0 to 177\n",
      "Data columns (total 14 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Class                         178 non-null    int64  \n",
      " 1   Alcohol                       178 non-null    float64\n",
      " 2   Malic acid                    178 non-null    float64\n",
      " 3   Ash                           178 non-null    float64\n",
      " 4   Alcalinity of ash             178 non-null    float64\n",
      " 5   Magnesium                     178 non-null    int64  \n",
      " 6   Total phenols                 178 non-null    float64\n",
      " 7   Flavanoids                    178 non-null    float64\n",
      " 8   Nonflavanoid phenols          178 non-null    float64\n",
      " 9   Proanthocyanins               178 non-null    float64\n",
      " 10  Color intensity               178 non-null    float64\n",
      " 11  Hue                           178 non-null    float64\n",
      " 12  OD280/OD315 of diluted wines  178 non-null    float64\n",
      " 13  Proline                       178 non-null    int64  \n",
      "dtypes: float64(11), int64(3)\n",
      "memory usage: 19.6 KB\n"
     ]
    }
   ],
   "source": [
    "wine_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fce466",
   "metadata": {},
   "source": [
    "- Fem la partició Train/Test del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f85ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realitzem la partició separant el 'target' i la resta de variables (13 en total)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = wine_df.drop(['Class'], axis=1) # eliminem la variable 'Class' i mantenim la resta de variables 'Alchol', 'Malic Acid', etc.\n",
    "y = wine_df['Class'] # Agafem només la variables 'Class'\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state =42) # Train = 80% / Test = 20%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f57b23ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptiu conjunt x_train:           Alcohol  Malic acid         Ash  Alcalinity of ash   Magnesium  \\\n",
      "count  142.000000  142.000000  142.000000         142.000000  142.000000   \n",
      "mean    12.979085    2.373521    2.360845          19.473239  100.443662   \n",
      "std      0.820116    1.143934    0.279217           3.454792   14.650793   \n",
      "min     11.030000    0.890000    1.360000          10.600000   70.000000   \n",
      "25%     12.332500    1.615000    2.210000          17.200000   88.250000   \n",
      "50%     13.010000    1.875000    2.360000          19.200000   98.000000   \n",
      "75%     13.677500    3.135000    2.540000          21.500000  107.000000   \n",
      "max     14.830000    5.800000    3.230000          30.000000  162.000000   \n",
      "\n",
      "       Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
      "count     142.000000  142.000000            142.000000       142.000000   \n",
      "mean        2.289085    2.002113              0.368028         1.608028   \n",
      "std         0.637715    1.004170              0.128269         0.583656   \n",
      "min         0.980000    0.340000              0.130000         0.420000   \n",
      "25%         1.725000    1.125000              0.270000         1.250000   \n",
      "50%         2.310000    2.075000              0.340000         1.555000   \n",
      "75%         2.800000    2.842500              0.470000         1.967500   \n",
      "max         3.880000    5.080000              0.660000         3.580000   \n",
      "\n",
      "       Color intensity         Hue  OD280/OD315 of diluted wines      Proline  \n",
      "count       142.000000  142.000000                    142.000000   142.000000  \n",
      "mean          5.057606    0.956380                      2.592817   734.894366  \n",
      "std           2.330917    0.234101                      0.722141   302.323595  \n",
      "min           1.740000    0.480000                      1.270000   278.000000  \n",
      "25%           3.220000    0.782500                      1.837500   502.500000  \n",
      "50%           4.600000    0.965000                      2.775000   660.000000  \n",
      "75%           6.122500    1.120000                      3.170000   932.750000  \n",
      "max          13.000000    1.710000                      4.000000  1547.000000  \n",
      "Descriptiu conjunt y_train: count    142.000000\n",
      "mean       1.964789\n",
      "std        0.775621\n",
      "min        1.000000\n",
      "25%        1.000000\n",
      "50%        2.000000\n",
      "75%        3.000000\n",
      "max        3.000000\n",
      "Name: Class, dtype: float64\n",
      "Descriptiu conjunt x_test:          Alcohol  Malic acid        Ash  Alcalinity of ash   Magnesium  \\\n",
      "count  36.000000   36.000000  36.000000          36.000000   36.000000   \n",
      "mean   13.085556    2.189722   2.388889          19.580556   96.972222   \n",
      "std     0.783641    1.005955   0.256713           2.882342   12.529932   \n",
      "min    11.410000    0.740000   1.700000          15.000000   78.000000   \n",
      "25%    12.370000    1.472500   2.255000          17.025000   88.000000   \n",
      "50%    13.380000    1.810000   2.425000          19.800000   96.000000   \n",
      "75%    13.662500    2.882500   2.567500          21.250000  106.500000   \n",
      "max    14.300000    4.600000   2.860000          25.000000  123.000000   \n",
      "\n",
      "       Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
      "count      36.000000   36.000000             36.000000        36.000000   \n",
      "mean        2.318889    2.136389              0.337500         1.523333   \n",
      "std         0.584601    0.984161              0.106217         0.527690   \n",
      "min         1.350000    0.500000              0.170000         0.410000   \n",
      "25%         1.935000    1.437500              0.267500         1.192500   \n",
      "50%         2.465000    2.310000              0.310000         1.545000   \n",
      "75%         2.812500    2.912500              0.412500         1.877500   \n",
      "max         3.300000    3.930000              0.630000         2.490000   \n",
      "\n",
      "       Color intensity        Hue  OD280/OD315 of diluted wines      Proline  \n",
      "count        36.000000  36.000000                     36.000000    36.000000  \n",
      "mean          5.060000   0.961667                      2.686111   794.222222  \n",
      "std           2.300308   0.208347                      0.664264   361.112813  \n",
      "min           1.280000   0.570000                      1.300000   325.000000  \n",
      "25%           3.222500   0.845000                      2.292500   501.500000  \n",
      "50%           4.900000   0.970000                      2.800000   680.000000  \n",
      "75%           6.412500   1.120000                      3.182500  1053.750000  \n",
      "max           9.899999   1.310000                      3.820000  1680.000000  \n",
      "Descriptiu conjunt y_train: count    36.000000\n",
      "mean      1.833333\n",
      "std       0.774597\n",
      "min       1.000000\n",
      "25%       1.000000\n",
      "50%       2.000000\n",
      "75%       2.000000\n",
      "max       3.000000\n",
      "Name: Class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Estudiem Train / Test a nivell descriptiu\n",
    "\n",
    "print(\"Descriptiu conjunt x_train:\", x_train.describe())\n",
    "\n",
    "print(\"Descriptiu conjunt y_train:\", y_train.describe())\n",
    "\n",
    "print(\"Descriptiu conjunt x_test:\", x_test.describe())\n",
    "\n",
    "print(\"Descriptiu conjunt y_train:\", y_test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac3333b",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3930d5bd",
   "metadata": {},
   "source": [
    "### Exercici 1\n",
    "\n",
    "Crea almenys dos models de classificació diferents per intentar predir el millor les classes de l'arxiu adjunt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc64bf60",
   "metadata": {},
   "source": [
    "#### - Arbre de Classificació (DecisionTreeClassifier)\n",
    "   - L'objectiu de l'arbre de classificació és dividir el conjunt de dades en subconjuts mes petits i homogenis.\n",
    "   - Cal controlar el sobreajust de les dades d'entrenament.\n",
    "   - Revisar i valorar si cal reduïr la profunditat máxima de l'arbre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba295155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, 1, 2, 1, 2, 3, 2, 3, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2,\n",
       "       2, 3, 3, 3, 2, 2, 2, 1, 1, 2, 3, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Creem l'arbre de classificació:\n",
    "\n",
    "DTC_wine = DecisionTreeClassifier()\n",
    "\n",
    "# Arbre de classificació del model 'Train'\n",
    "\n",
    "DTC_wine = DTC_wine.fit(x_train,y_train)\n",
    "\n",
    "# Per predir la resposta per al model de 'Test'\n",
    "\n",
    "pred_DTC_wine = DTC_wine.predict(x_test)\n",
    "\n",
    "pred_DTC_wine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242e6fe9",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62a8f03",
   "metadata": {},
   "source": [
    "#### - SVM: Support Vector Machines\n",
    "   - S'utilitza per classificar les dades en dues o més clases\n",
    "   - Divideix l'espai de les dades en dues parts utiltzant un hiperplà.\n",
    "   - El 'marge' es la distancia entre els punts mes propers de les dues classes.\n",
    "   - L'objectiu del SVM es torbar un hiperplà amb el major marge possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45282c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 1, 2, 1, 2, 3, 3, 3, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2,\n",
       "       3, 3, 2, 2, 2, 3, 2, 1, 1, 2, 3, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Creem el model SVM\n",
    "\n",
    "SVM_wine = svm.SVC() # kernel\n",
    "\n",
    "# Entrenem el model 'Train'\n",
    "\n",
    "SVM_wine.fit(x_train, y_train)\n",
    "\n",
    "# Per predir la resposta per al model de 'Test\n",
    "\n",
    "pred_SVM_wine = SVM_wine.predict(x_test)\n",
    "\n",
    "pred_SVM_wine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9cf477",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a151a8f9",
   "metadata": {},
   "source": [
    "### Exercici 2\n",
    "\n",
    "Compara els models de classificació utilitzant la precisió (accuracy), una matriu de confusió i d’altres mètriques més avançades."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d55c8",
   "metadata": {},
   "source": [
    " ##### - Classification Acuracy: percentatge de prediccions correctes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cb482ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>0.805556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy\n",
       "0  Decision Tree Classifier  0.944444\n",
       "1   Support Vector Machines  0.805556"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "accuracy_DTC_wine = metrics.accuracy_score(y_test, pred_DTC_wine)\n",
    "accuracy_SVM_wine = metrics.accuracy_score(y_test, pred_SVM_wine)\n",
    "\n",
    "# Realitzem un DF per visualitzar els resultats\n",
    "\n",
    "accuracy_df = pd.DataFrame(\n",
    "    data = [('Decision Tree Classifier',accuracy_DTC_wine), ('Support Vector Machines',accuracy_SVM_wine)],\n",
    "        columns = ['Model', 'Accuracy'])\n",
    "\n",
    "accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0cee80",
   "metadata": {},
   "source": [
    "Comentaris:\n",
    "\n",
    "- Els model DTC té un alt percentatge de prediccions correctes (94%), mentre que el model SVM te un accuracy sensiblement inferior (80%)\n",
    "- En aquest cas, el model 'Decision Trees Classifier' sería més precís que el SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97d83c2",
   "metadata": {},
   "source": [
    " ##### - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9f577cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13,  1,  0],\n",
       "       [ 0, 14,  0],\n",
       "       [ 1,  0,  7]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "cm_DTC_wine = metrics.confusion_matrix(y_test, pred_DTC_wine)\n",
    "\n",
    "cm_DTC_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60095346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14,  0,  0],\n",
       "       [ 0, 11,  3],\n",
       "       [ 0,  4,  4]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_SVM_wine = metrics.confusion_matrix(y_test, pred_SVM_wine)\n",
    "\n",
    "cm_SVM_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3600ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred.DTC Class 1</th>\n",
       "      <th>Pred.DTC Class 2</th>\n",
       "      <th>Pred.DTC Class 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 2</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Pred.DTC Class 1  Pred.DTC Class 2  Pred.DTC Class 3\n",
       "Class 1                13                 1                 0\n",
       "Class 2                 0                14                 0\n",
       "Class 3                 1                 0                 7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pasem la Confusion Matrix del model DecisionTreeClassifier a DataFrame\n",
    "\n",
    "cm_DTC_wine_DF = pd.DataFrame(cm_DTC_wine, index = ['Class 1', 'Class 2', 'Class 3'], columns = ['Pred.DTC Class 1', 'Pred.DTC Class 2', 'Pred.DTC Class 3'])\n",
    "\n",
    "cm_DTC_wine_DF\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c959c9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred.SVM Class 1</th>\n",
       "      <th>Pred.SVM Class 2</th>\n",
       "      <th>Pred.SVM Class 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 2</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Pred.SVM Class 1  Pred.SVM Class 2  Pred.SVM Class 3\n",
       "Class 1                14                 0                 0\n",
       "Class 2                 0                11                 3\n",
       "Class 3                 0                 4                 4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pasem la Confusion Matrix del model SVM a DataFrame\n",
    "\n",
    "cm_SVM_wine_DF = pd.DataFrame(cm_SVM_wine, index = ['Class 1', 'Class 2', 'Class 3'], columns = ['Pred.SVM Class 1', 'Pred.SVM Class 2', 'Pred.SVM Class 3'])\n",
    "\n",
    "cm_SVM_wine_DF\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481870d4",
   "metadata": {},
   "source": [
    "Comentaris:\n",
    "\n",
    "- Els model DTC (DecisionTreeClassfier) ha tingut errors en la predicció de la Class 1 i la Class 3, acomulant un total de 2 errors.\n",
    "- Els model SVM (Support Vector Machines) ha tingut errors en la predicció de la Class 2 i la Class 3, acomulant un total de 7 errors.\n",
    "- Podriem dir que, com en el cas de 'Accuracy', el model DTC sembla més precís que el model SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95e3134",
   "metadata": {},
   "source": [
    " ##### - Logistic Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1db2fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic loss DTC: 1.9188209108283725\n",
      "Logistic loss SVM: 0.4801887582914907\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Calculem logistic loss del model DTC (DecisionTreeModel)\n",
    "\n",
    "Loss_DTC_wine = log_loss(y_test, DTC_wine.predict_proba(x_test))\n",
    "\n",
    "# Calculem logistic loss del model SVM (Support Vector Machines)\n",
    "\n",
    "SVM_wine = svm.SVC(probability=True)\n",
    "\n",
    "SVM_wine.fit(x_train, y_train)\n",
    "\n",
    "Loss_SVM_wine = log_loss(y_test, SVM_wine.predict_proba(x_test))\n",
    "\n",
    "print('Logistic loss DTC:', Loss_DTC_wine)\n",
    "print('Logistic loss SVM:', Loss_SVM_wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91db3964",
   "metadata": {},
   "source": [
    "Comentaris:\n",
    "\n",
    "- Amb 'log_loss' es tracta d'obtenir la probabilitat de la similitud entre els valors predictius (probabilitats) i els valors reals.\n",
    "- Utilitzem 'predict_proba' per obtenir les probabilitats de predicció del model.\n",
    "- A menor valor de 'log_loss', millor serán les prediccions del model.\n",
    "- En aques cas, podríem dir que les prediccions del model SVM podríen ser millors respecte a les del model DTC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d595564a",
   "metadata": {},
   "source": [
    "##### - Clasification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5085cf3b",
   "metadata": {},
   "source": [
    "- Precision: Percentatge de prediccions positives correctes respecte el total de prediccions positives.\n",
    "\n",
    "- Recall: Percentatge de prediccions positives correctes en relació amb el total de positius reals.\n",
    "\n",
    "- F1 Score: Mitjana harmònica ponderada de 'Precision' i ' Recall'. A més proximitat al 1, millor és el model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c96ead0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.929     0.929     0.929        14\n",
      "           2      0.933     1.000     0.966        14\n",
      "           3      1.000     0.875     0.933         8\n",
      "\n",
      "    accuracy                          0.944        36\n",
      "   macro avg      0.954     0.935     0.942        36\n",
      "weighted avg      0.946     0.944     0.944        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clasificatcion Report del model DTC (DecisionTreeClassifier)\n",
    "\n",
    "print(metrics.classification_report(y_test, pred_DTC_wine, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6767cd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      1.000     1.000     1.000        14\n",
      "           2      0.733     0.786     0.759        14\n",
      "           3      0.571     0.500     0.533         8\n",
      "\n",
      "    accuracy                          0.806        36\n",
      "   macro avg      0.768     0.762     0.764        36\n",
      "weighted avg      0.801     0.806     0.802        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clasificatcion Report del model SVM (Support Vector Machines)\n",
    "\n",
    "print(metrics.classification_report(y_test, pred_SVM_wine, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f57ef2",
   "metadata": {},
   "source": [
    "Comentaris:\n",
    "\n",
    "- Amb el 'Clasification Report' obtenim les mètriques de classificació del nostres model.\n",
    "- Precision:\n",
    "    - 'DecissionTreeClassifier':\n",
    "        - la precisió per a la classe 1 és del 93%, indica que algunes mostres classificades com a classe 1 son en realitat d'una altra classe. Passa el mateix amb la classe 2, amb una precisió del 93%. Per a la classe 3, la precisió és del 100%, el que indica que totes les mostres classificades com a classe 3 son realment d'aquesta classe.\n",
    "    - 'Support Vector Machines':\n",
    "        - la precisió per a la classe 1 és del 100%, totes les mostres classificades com a classe 1 son realment d'aquesta classe. La precisió de la classe 2, és del 73% el que indica que algunes mostres classificades com a classe 2 son en realitat d'una altra classe. Per a la classe 3, la precisió és del 57%, el que indica que moltes de les mostres classificades com a classe 3 son en d'una altra classe. \n",
    "- Recall (Sensibilitat):\n",
    "    - 'DecissionTreeClassifier':\n",
    "        - El recall per a la classe 1 és del 93%, el que significa que algunes mostres de la classe 1 s'han classificat incorrectament com a una altra classe. Per a la classe 2, el recall és del 100%, el que indica que totes les mostres de la classe 2 s'han classificat correctament. La classe 3, el recall és del 87,5%, com en el cas de la classe 1 algunes mostres s'han classificat incorrectament.\n",
    "    - 'Support Vector Machines':\n",
    "         - El recall per a la classe 1 és del 100%, el que indica que totes les mostres de la classe 1 s'han classificat correctament. Per a la classe 2, el recall és del 79%, indica que algunes mostres de la classe 2 s'han classificat incorrectament com a una altra classe. La classe 3, el recall és del 50%, moltes de les mostres s'han classificat incorrectament.\n",
    "- Accuracy: mesura la proporció de mostres classificades correctament.\n",
    "    - 'DecissionTreeClassifier':\n",
    "         - l'Accuracy global del model és del 94%. Podríem dir que el model té un bon rendiment en la classificació de les 3 classes.\n",
    "    - 'Support Vector Machines':\n",
    "         - En aquest cas, el valor és del 80%. Aquest model pdoríem dir que no presenta tant bon rendiment en la classificació de les 3 classes, respecte al model de DecissionTreeClassifier.\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3378b610",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd3d0e",
   "metadata": {},
   "source": [
    "### Exercici 3\n",
    "   Entrena’ls usant els diferents paràmetres que admeten per tal de millorar-ne la predicció."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b1e867",
   "metadata": {},
   "source": [
    "##### 1- DecisionTreeClassifier\n",
    "\n",
    "- Inicialment utilitzem GridSearchCV per trobar el millors paràmetres del model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85bdd032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Els millors hiperparàmtres son: {'criterion': 'gini', 'max_depth': 4, 'min_samples_split': 2}\n",
      "El 'score' amb els millors hiperparàmetres és: 0.9224137931034484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RSIMONNE\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "125 fits failed out of a total of 250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "125 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\RSIMONNE\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\RSIMONNE\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\RSIMONNE\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'antropy'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\RSIMONNE\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.87315271 0.87315271 0.87315271 0.87315271 0.87315271 0.92241379\n",
      " 0.92241379 0.91527094 0.92241379 0.92241379 0.91527094 0.91527094\n",
      " 0.92241379 0.92241379 0.92241379 0.92241379 0.92241379 0.92241379\n",
      " 0.91527094 0.91527094 0.91527094 0.92241379 0.91527094 0.92241379\n",
      " 0.92241379        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definim el model\n",
    "\n",
    "DTC_wine #utilitzem el model anteriorment creat (arbre de classificació)\n",
    "\n",
    "# Definim la llista de hierparamtres a ajustar i els valors a provar:\n",
    "\n",
    "param_grid = {'max_depth': [2,4,6,8,10], 'criterion': ['gini', 'antropy'], 'min_samples_split': [2,4,6,8,10]}\n",
    "\n",
    "# CrossValidation \n",
    "\n",
    "cv = 5\n",
    "\n",
    "    #'max_depth': Profunditat máxima de l'arbre. L'arbre es dividirà fins el valor màxim definit o fins que les fulles siguin pures.\n",
    "                    # Limitar la profunditat màxima pot ajudar a prevenir el sobreajust del model.\n",
    "\n",
    "    #'Criterion': Criteris disponibles per als arbres de decissió (impuressa i incertessa del conjunt de dades)\n",
    "                # 'Gini': La impuresa de 'Gini' es defineix com la probabilitat de que dos elements seleccionats aleatoriament d'un conjunt de dades siguin de diferents classes.\n",
    "                            # Un valor de 'Gini' proper a '0' indica que el conjunt de dades és més homogeni.\n",
    "                # 'Antropy': Es defineix com la mesura d'incertessa d'una variable aleatoria.En arbres de decissió s'utilitza l'antropia per medir la quantitat d'informació que es guanya després de realitzar la divisió.\n",
    "                            # A subconjunts més homogenis, antropia menor. A subconjunt més heterogenis, antropia major.\n",
    "    #'mini_sample_split': nº mínim de mostres que es necessiten per dividir un node intern de l'arbre.\n",
    "    \n",
    "    #'CV' (CrossValidation): permet evaluar el rendiement del model i evitar el sobreajustament.\n",
    "                            # Especificant el valor de CV, fixem el número de 'folds'. Iteracions realitzades en el model\n",
    "    \n",
    "    # Els parametres 'max_depht' i 'mini_sample_split' son importants en la construcció d'un model d'arbre de decissió òptim.\n",
    "    # S'han d'ajustar mitjançant validació creuada.\n",
    "\n",
    "# Definim la GridSearch\n",
    "\n",
    "grid_search_DTC_wine = GridSearchCV(DTC_wine, param_grid, cv=cv)\n",
    "\n",
    "# Entrenem el model amb les dades d'entrenament\n",
    "\n",
    "grid_search_DTC_wine.fit(x_train,y_train)\n",
    "\n",
    "# Obtenim els millors parametres i el 'score' del model\n",
    "\n",
    "best_params = grid_search_DTC_wine.best_params_\n",
    "best_score = grid_search_DTC_wine.best_score_\n",
    "\n",
    "print(\"Els millors hiperparàmtres son:\", best_params)\n",
    "print(\"El 'score' amb els millors hiperparàmetres és:\", best_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2886fda4",
   "metadata": {},
   "source": [
    "- Fem el nou model utilitzant els millors hiperparàmtres obtinguts i l'entrenem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5be7d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generem el nou model i l'entrenem\n",
    "\n",
    "DTC_wine_2 = DecisionTreeClassifier(criterion= 'gini', max_depth = 4, min_samples_split= 2)\n",
    "DTC_wine_2.fit(x_train,y_train)\n",
    "pred_DTC_wine_2 = DTC_wine_2.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16747fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculem la 'Accuracy', la 'ConfusionMatrix' i 'LogisticLoss' del nou model per poder comparar-ho amb el model incial\n",
    "\n",
    "accuracy_DTC_wine_2 = metrics.accuracy_score(y_test, pred_DTC_wine_2)\n",
    "\n",
    "cm_DTC_wine_2 = metrics.confusion_matrix(y_test, pred_DTC_wine_2)\n",
    "\n",
    "Loss_DTC_wine_2 = log_loss(y_test, DTC_wine_2.predict_proba(x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf9822e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Log_Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DTC Incicial</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>1.918821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DTC Paràmetres</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>1.918821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model  Accuracy  Log_Loss\n",
       "0    DTC Incicial  0.944444  1.918821\n",
       "1  DTC Paràmetres  0.944444  1.918821"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generem un DF per visualitzar millor les dades a comparar dels dos models, inicial i entrenat amb paràmetres (Accuracy + Log_Loss)\n",
    "\n",
    "DTC_wine_comp = pd.DataFrame( data = [('DTC Incicial', accuracy_DTC_wine, Loss_DTC_wine), \n",
    "                                  ('DTC Paràmetres', accuracy_DTC_wine_2, Loss_DTC_wine_2)],\n",
    "                         columns = ['Model', 'Accuracy', 'Log_Loss'])\n",
    "DTC_wine_comp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acb58ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfusionMatrix model DTC entrenat amb paràmetres:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred.DTC_2 Class 1</th>\n",
       "      <th>Pred.DTC_2 Class 2</th>\n",
       "      <th>Pred.DTC_2 Class 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 2</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Pred.DTC_2 Class 1  Pred.DTC_2 Class 2  Pred.DTC_2 Class 3\n",
       "Class 1                  13                   1                   0\n",
       "Class 2                   0                  14                   0\n",
       "Class 3                   0                   1                   7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generem un DF de la confusion matrix del model entrenat amb paràmetres\n",
    "\n",
    "cm_DTC_wine_2_DF = pd.DataFrame(cm_DTC_wine_2, index = ['Class 1', 'Class 2', 'Class 3'],\n",
    "                                 columns = ['Pred.DTC_2 Class 1', 'Pred.DTC_2 Class 2', 'Pred.DTC_2 Class 3'])\n",
    "\n",
    "print('ConfusionMatrix model DTC entrenat amb paràmetres:')\n",
    "\n",
    "cm_DTC_wine_2_DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4911974d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfusionMatrix model DTC inicial:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred.DTC Class 1</th>\n",
       "      <th>Pred.DTC Class 2</th>\n",
       "      <th>Pred.DTC Class 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 2</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Pred.DTC Class 1  Pred.DTC Class 2  Pred.DTC Class 3\n",
       "Class 1                13                 1                 0\n",
       "Class 2                 0                14                 0\n",
       "Class 3                 1                 0                 7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i la comparem amb la del model incial.\n",
    "\n",
    "print('ConfusionMatrix model DTC inicial:')\n",
    "\n",
    "cm_DTC_wine_DF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e3697",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c84e58",
   "metadata": {},
   "source": [
    "##### 2- SVM: Support Vector Machines\n",
    "\n",
    "- Inicialment utilitzem GridSearchCV per trobar el millors paràmetres del model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ded7b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Els millors hiperparàmtres son: {'C': 100, 'degree': 1, 'gamma': 'scale', 'probability': True}\n",
      "El 'score' amb els millors hiperparàmetres és: 0.7187192118226602\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Definim el model\n",
    "\n",
    "SVM_wine #utilitzem el model anteriorment creat (arbre de classificació)\n",
    "\n",
    "# Definim la llista de hierparamtres a ajustar i els valors a provar:\n",
    "\n",
    "param_grid_SVM_wine = {'C': [0.01, 0.1, 1, 10, 100],'gamma': ['scale', 'auto'], 'degree': [1,2,3,4,5],'probability': [True]}\n",
    "\n",
    "cv = 5\n",
    "\n",
    "    #'C': Paràmtre de regulació de SVM, que controla la compensació (trade-off) entre la precissió en la classificació de les dades d'entrenament i la simplicitat del model. \n",
    "                    # Un valor mes alt de 'C', incrementa la precisió en la classificació de les dades d'entrenament, però pot dur a un sobreajustament del model.\n",
    "    #'Gamma': Defineix la influencia de cada punt d'entrenament en la definició del límit de decissió.\n",
    "                    # Un valor mes alt de 'gamma' fa que el model sigui mes sensible a punts propers, però pot dur a sobreajustament.\n",
    "                        # 'scale': 1 / (n_features * X.var()) Escala les característiques a mitjana '0' i variança unitaria\n",
    "                        # ' auto': 1 / n_features\n",
    "    #'degree': Defineix el grau del polinomi utilitzat per transformar les dades d'entrada en un espai de característiques de major dimensió.\n",
    "                    # Un valor mes alt de 'degree' pot incremnetar la complexitat del model.\n",
    "    #'probability': habilitant aquesta opció, en lloc de simplement predir la classe de la mostra d'entrada, el model també pot estimar la probabilitat de que la mostra pertanyi a cada classe.\n",
    "    \n",
    "    #'CV' (CrossValidation): permet evaluar el rendiement del model i evitar el sobreajustament.\n",
    "                            # Especificant el valor de CV, fixem el número de 'folds'. Iteracions realitzades en el model\n",
    "    \n",
    "# Definim la GridSearch\n",
    "\n",
    "grid_search_SVM_wine = GridSearchCV(SVM_wine, param_grid_SVM_wine, cv=cv)\n",
    "\n",
    "# Entrenem el model amb les dades d'entrenament\n",
    "\n",
    "grid_search_SVM_wine.fit(x_train,y_train)\n",
    "\n",
    "# Obtenim els millors parametres i el 'score' del model\n",
    "\n",
    "best_params = grid_search_SVM_wine.best_params_\n",
    "best_score = grid_search_SVM_wine.best_score_\n",
    "\n",
    "print(\"Els millors hiperparàmtres son:\", best_params)\n",
    "print(\"El 'score' amb els millors hiperparàmetres és:\", best_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee0da5",
   "metadata": {},
   "source": [
    "- Fem el nou model utilitzant els millors hiperparàmtres obtinguts i l'entrenem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "691bc086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generem el nou model i l'entrenem\n",
    "\n",
    "SVM_wine_2 = svm.SVC(C = 100, degree = 1, gamma = 'scale', probability = True)\n",
    "SVM_wine_2.fit(x_train,y_train)\n",
    "pred_SVM_wine_2 = SVM_wine_2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5c9fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculem la 'Accuracy', la 'ConfusionMatrix' i 'LogisticLoss' del nou model per poder comparar-ho amb el model incial\n",
    "\n",
    "accuracy_SVM_wine_2 = metrics.accuracy_score(y_test, pred_SVM_wine_2)\n",
    "\n",
    "cm_SVM_wine_2 = metrics.confusion_matrix(y_test, pred_SVM_wine_2)\n",
    "\n",
    "Loss_SVM_wine_2 = log_loss(y_test, SVM_wine_2.predict_proba(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44caf02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Log_Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM Incicial</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.480189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM Paràmetres</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.419474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model  Accuracy  Log_Loss\n",
       "0    SVM Incicial  0.805556  0.480189\n",
       "1  SVM Paràmetres  0.833333  0.419474"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generem un DF per visualitzar millor les dades a comparar, dels dos models, inicial i entrenat amb paràmetres (Accuracy + Log_Loss)\n",
    "\n",
    "SVM_wine_comp = pd.DataFrame( data = [('SVM Incicial', accuracy_SVM_wine, Loss_SVM_wine), \n",
    "                                  ('SVM Paràmetres', accuracy_SVM_wine_2, Loss_SVM_wine_2)],\n",
    "                         columns = ['Model', 'Accuracy', 'Log_Loss'])\n",
    "SVM_wine_comp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d9814e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfusionMatrix model SVM entrenat amb paràmetres:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred.SVM_2 Class 1</th>\n",
       "      <th>Pred.SVM_2 Class 2</th>\n",
       "      <th>Pred.SVM_2 Class 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 2</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Pred.SVM_2 Class 1  Pred.SVM_2 Class 2  Pred.SVM_2 Class 3\n",
       "Class 1                  14                   0                   0\n",
       "Class 2                   0                  10                   4\n",
       "Class 3                   1                   1                   6"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generem un DF de la confusion matrix del model entrenat amb paràmetres\n",
    "\n",
    "cm_SVM_wine_2_DF = pd.DataFrame(cm_SVM_wine_2, index = ['Class 1', 'Class 2', 'Class 3'],\n",
    "                                 columns = ['Pred.SVM_2 Class 1', 'Pred.SVM_2 Class 2', 'Pred.SVM_2 Class 3'])\n",
    "\n",
    "print('ConfusionMatrix model SVM entrenat amb paràmetres:')\n",
    "\n",
    "cm_SVM_wine_2_DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b9696d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfusionMatrix model SVM inicial:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred.SVM Class 1</th>\n",
       "      <th>Pred.SVM Class 2</th>\n",
       "      <th>Pred.SVM Class 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 2</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Pred.SVM Class 1  Pred.SVM Class 2  Pred.SVM Class 3\n",
       "Class 1                14                 0                 0\n",
       "Class 2                 0                11                 3\n",
       "Class 3                 0                 4                 4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i la comparem amb la del model incial.\n",
    "\n",
    "print('ConfusionMatrix model SVM inicial:')\n",
    "\n",
    "cm_SVM_wine_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e1b537",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f49464",
   "metadata": {},
   "source": [
    "### Exercici 4\n",
    " \n",
    "Compara el seu rendiment fent servir l’aproximació traint/test o cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d63279a",
   "metadata": {},
   "source": [
    "- Comparem els diferents models generats amb cross-validation (models inicials i models entrenats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21a5c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicció del 'score' de 'cross validation'\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_DTC_wine = cross_val_score(estimator = DTC_wine, X = x, y = y, cv = 4)\n",
    "cv_SVM_wine = cross_val_score(estimator = SVM_wine, X = x, y = y, cv = 4)\n",
    "cv_DTC_wine_2 = cross_val_score(estimator = DTC_wine_2, X = x, y = y, cv = 4)\n",
    "cv_SVM_wine_2 = cross_val_score(estimator = SVM_wine_2, X = x, y = y, cv = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b561e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Cross-validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DTC Paràmetres</td>\n",
       "      <td>0.893813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DTC Inicial</td>\n",
       "      <td>0.859722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM Paràmetres</td>\n",
       "      <td>0.736237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM Inicial</td>\n",
       "      <td>0.663131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model  Cross-validation\n",
       "1  DTC Paràmetres          0.893813\n",
       "0     DTC Inicial          0.859722\n",
       "3  SVM Paràmetres          0.736237\n",
       "2     SVM Inicial          0.663131"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generem un DF per comparar els diferents valors obtinguts de la 'cross-validation'\n",
    "\n",
    "cv_comp = pd.DataFrame( data = [('DTC Inicial', cv_DTC_wine.mean()),\n",
    "                                ('DTC Paràmetres', cv_DTC_wine_2.mean()),\n",
    "                                ('SVM Inicial', cv_SVM_wine.mean()),\n",
    "                                ('SVM Paràmetres', cv_SVM_wine_2.mean())],\n",
    "                         columns = ['Model', 'Cross-validation'])\n",
    "\n",
    "cv_comp = cv_comp.sort_values(by=['Cross-validation'], ascending=False)\n",
    "\n",
    "cv_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ad7923",
   "metadata": {},
   "source": [
    "Comentaris:\n",
    "\n",
    "- Havent realitzat 'cross-validation' dels dieferents models generats (inicials i amb paràmetres) podríem dir que el model mes ajustat es el __DecisionTreeClassifier__ __amb__ __paràmtres__ (DTC Paràmetres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e41382",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912ee0b1",
   "metadata": {},
   "source": [
    "### Exercici 5\n",
    "   Aplica algun procés d'enginyeria per millorar els resultats (normalització, estandardització, mostreig...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddb73dd",
   "metadata": {},
   "source": [
    "Aplicarem aquests processos al model que hem considerat més ajustat: __DecisionTreeClassifier__ __amb__ __paràmtres__ (DTC Paràmetres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84691834",
   "metadata": {},
   "source": [
    "- __Normalització / Estandarització__: Transforma les dades per a que totes les variables tinguin un rang enre 0 i 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17057279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalitzem les dades\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_train_norm = scaler.fit_transform(x_train)\n",
    "x_test_norm = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "def54564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, 1, 2, 1, 2, 3, 2, 3, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2,\n",
       "       2, 3, 3, 3, 2, 2, 2, 1, 1, 2, 3, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenem el model\n",
    "\n",
    "DTC_wine_2_norm = DTC_wine_2.fit(x_train_norm,y_train)\n",
    "pred_DTC_wine_2_norm = DTC_wine_2_norm.predict(x_test_norm)\n",
    "\n",
    "pred_DTC_wine_2_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83c27c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9444444444444444\n",
      "log_loss: 1.9188209108283725\n",
      "Cross-validation: 0.9214646464646465\n"
     ]
    }
   ],
   "source": [
    "# Calculem 'Accuracy', log_loss' i fem 'Cross-validation'\n",
    "\n",
    "accuracy_DTC_wine_2_norm = metrics.accuracy_score(y_test, pred_DTC_wine_2_norm)\n",
    "\n",
    "print('accuracy:', accuracy_DTC_wine_2_norm)\n",
    "\n",
    "Loss_DTC_wine_2_norm = log_loss(y_test, DTC_wine_2_norm.predict_proba(x_test_norm))\n",
    "\n",
    "print('log_loss:', Loss_DTC_wine_2_norm)\n",
    "\n",
    "cv_DTC_wine_2_norm = cross_val_score(estimator = DTC_wine_2_norm, X = x, y = y, cv = 4)\n",
    "\n",
    "print('Cross-validation:', cv_DTC_wine_2_norm.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7803d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred.NORM Class 1</th>\n",
       "      <th>Pred.NORM Class 2</th>\n",
       "      <th>Pred.NORM Class 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 2</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Pred.NORM Class 1  Pred.NORM Class 2  Pred.NORM Class 3\n",
       "Class 1                 13                  1                  0\n",
       "Class 2                  0                 14                  0\n",
       "Class 3                  0                  1                  7"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculem la confusion matrix\n",
    "\n",
    "cm_DTC_wine_2_norm = metrics.confusion_matrix(y_test, pred_DTC_wine_2_norm)\n",
    "\n",
    "cm_DTC_wine_2_norm\n",
    "\n",
    "# la presentem en DF\n",
    "\n",
    "cm_DTC_wine_2_norm_DF = pd.DataFrame(cm_DTC_wine_2_norm, index = ['Class 1', 'Class 2', 'Class 3'], columns = ['Pred.NORM Class 1', 'Pred.NORM Class 2', 'Pred.NORM Class 3'])\n",
    "\n",
    "cm_DTC_wine_2_norm_DF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78510b4d",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3030123f",
   "metadata": {},
   "source": [
    "- __Selecció millors features__: Selecciona les millors features (característiques) de les dades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ba0c803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionem les 10 millors features\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif  # evalua la relació entre cada variable i la variable objectiu\n",
    "\n",
    "selector = SelectKBest(f_classif, k=10)  # K: nº de millors features que volem.\n",
    "x_train_sel = selector.fit_transform(x_train, y_train)\n",
    "x_test_sel = selector.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5739efd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, 1, 2, 1, 2, 3, 2, 3, 1, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2,\n",
       "       2, 3, 3, 3, 2, 2, 2, 1, 1, 2, 3, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenem el model\n",
    "\n",
    "DTC_wine_2_sel = DTC_wine_2.fit(x_train_sel,y_train)\n",
    "pred_DTC_wine_2_sel = DTC_wine_2_norm.predict(x_test_sel)\n",
    "\n",
    "pred_DTC_wine_2_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7feb1488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9722222222222222\n",
      "log_loss: 0.9594104554141872\n",
      "Cross-validation: 0.9271464646464647\n"
     ]
    }
   ],
   "source": [
    "# Calculem 'Accuracy', log_loss' i fem 'Cross-validation'\n",
    "\n",
    "accuracy_DTC_wine_2_sel = metrics.accuracy_score(y_test, pred_DTC_wine_2_sel)\n",
    "\n",
    "print('accuracy:', accuracy_DTC_wine_2_sel)\n",
    "\n",
    "Loss_DTC_wine_2_sel = log_loss(y_test, DTC_wine_2_sel.predict_proba(x_test_sel))\n",
    "\n",
    "print('log_loss:', Loss_DTC_wine_2_sel)\n",
    "\n",
    "cv_DTC_wine_2_sel = cross_val_score(estimator = DTC_wine_2_sel, X = x, y = y, cv = 4)\n",
    "\n",
    "print('Cross-validation:', cv_DTC_wine_2_sel.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33b84edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred.SEL Class 1</th>\n",
       "      <th>Pred.SEL Class 2</th>\n",
       "      <th>Pred.SEL Class 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 2</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Pred.SEL Class 1  Pred.SEL Class 2  Pred.SEL Class 3\n",
       "Class 1                14                 0                 0\n",
       "Class 2                 0                14                 0\n",
       "Class 3                 1                 0                 7"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculem la confusion matrix\n",
    "\n",
    "cm_DTC_wine_2_sel = metrics.confusion_matrix(y_test, pred_DTC_wine_2_sel)\n",
    "\n",
    "cm_DTC_wine_2_sel\n",
    "\n",
    "# la presentem en DF\n",
    "\n",
    "cm_DTC_wine_2_sel_DF = pd.DataFrame(cm_DTC_wine_2_sel, index = ['Class 1', 'Class 2', 'Class 3'], columns = ['Pred.SEL Class 1', 'Pred.SEL Class 2', 'Pred.SEL Class 3'])\n",
    "\n",
    "cm_DTC_wine_2_sel_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df6ffcd",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a235a8",
   "metadata": {},
   "source": [
    "- __Mostreig__: Si al model hi ha més valors d'una classe que d'una altra, amb el mostreig podem aconseguir que el model no es decanti cap a la classe majoritària i donar més pes a la classe minoritària."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11c97ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# amb 'random under-sampling' s'aconsegueix reduïr el nombre d'instànices de la 'classe' majoritària fins a un nivell igual al de la 'classe' minoritària.\n",
    "\n",
    "# Definim l'estartgeia:\n",
    "mostreig = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "\n",
    "# Apliquem 'under-sampling' al model d'entrenament:\n",
    "x_train_mostreig, y_train_mostreig = mostreig.fit_resample(x_train, y_train)\n",
    "\n",
    "x_train_mostreig, x_test_mostreig, y_train_mostreig, y_test_mostreig = train_test_split(x_train_mostreig, y_train_mostreig, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca5eeb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 2, 2, 3, 2, 3, 3, 1, 1, 3, 1, 3, 1, 2, 1, 3, 1, 3, 2, 2,\n",
       "       1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenem el model\n",
    "\n",
    "DTC_wine_2_mostreig = DTC_wine_2.fit(x_train_mostreig, y_train_mostreig)\n",
    "pred_DTC_wine_2_mostreig = DTC_wine_2_mostreig.predict(x_test_mostreig)\n",
    "\n",
    "pred_DTC_wine_2_mostreig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ddbd33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.96\n",
      "log_loss: 1.3815510557964295\n",
      "Cross-validation: 0.9272727272727272\n"
     ]
    }
   ],
   "source": [
    "# Calculem 'Accuracy', log_loss' i fem 'Cross-validation'\n",
    "\n",
    "accuracy_DTC_wine_2_mostreig = metrics.accuracy_score(y_test_mostreig, pred_DTC_wine_2_mostreig)\n",
    "\n",
    "print('accuracy:', accuracy_DTC_wine_2_mostreig)\n",
    "\n",
    "Loss_DTC_wine_2_mostreig = log_loss(y_test_mostreig, DTC_wine_2_mostreig.predict_proba(x_test_mostreig))\n",
    "\n",
    "print('log_loss:', Loss_DTC_wine_2_mostreig)\n",
    "\n",
    "cv_DTC_wine_2_mostreig = cross_val_score(estimator = DTC_wine_2_mostreig, X = x, y = y, cv = 4)\n",
    "\n",
    "print('Cross-validation:', cv_DTC_wine_2_mostreig.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9067bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred.MOSTREIG Class 1</th>\n",
       "      <th>Pred.MOSTREIG Class 2</th>\n",
       "      <th>Pred.MOSTREIG Class 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 2</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Pred.MOSTREIG Class 1  Pred.MOSTREIG Class 2  Pred.MOSTREIG Class 3\n",
       "Class 1                     11                      1                      0\n",
       "Class 2                      0                      6                      0\n",
       "Class 3                      0                      0                      7"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculem la confusion matrix\n",
    "\n",
    "cm_DTC_wine_2_mostreig = metrics.confusion_matrix(y_test_mostreig, pred_DTC_wine_2_mostreig)\n",
    "\n",
    "cm_DTC_wine_2_mostreig\n",
    "\n",
    "# la presentem en DF\n",
    "\n",
    "cm_DTC_wine_2_mostreig_DF = pd.DataFrame(cm_DTC_wine_2_mostreig, index = ['Class 1', 'Class 2', 'Class 3'], columns = ['Pred.MOSTREIG Class 1', 'Pred.MOSTREIG Class 2', 'Pred.MOSTREIG Class 3'])\n",
    "\n",
    "cm_DTC_wine_2_mostreig_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605f7921",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ceb7c6",
   "metadata": {},
   "source": [
    "### RESUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eae3d5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Log_Loss</th>\n",
       "      <th>Cross-Validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DecisionTreeClassifier Entrenat Selecció:</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.959410</td>\n",
       "      <td>0.927146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DecisionTreeClassifier Entrenat Mostreig:</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>1.381551</td>\n",
       "      <td>0.927273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier:</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>1.918821</td>\n",
       "      <td>0.859722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTreeClassifier Entrenat:</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>1.918821</td>\n",
       "      <td>0.893813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTreeClassifier Entrenat Normalitzat:</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>1.918821</td>\n",
       "      <td>0.921465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Support Vector Machines Entrenat:</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.419474</td>\n",
       "      <td>0.736237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Support Vector Machines:</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.480189</td>\n",
       "      <td>0.663131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          model  Accuracy  Log_Loss  \\\n",
       "5     DecisionTreeClassifier Entrenat Selecció:  0.972222  0.959410   \n",
       "6     DecisionTreeClassifier Entrenat Mostreig:  0.960000  1.381551   \n",
       "0                       DecisionTreeClassifier:  0.944444  1.918821   \n",
       "2              DecisionTreeClassifier Entrenat:  0.944444  1.918821   \n",
       "4  DecisionTreeClassifier Entrenat Normalitzat:  0.944444  1.918821   \n",
       "3             Support Vector Machines Entrenat:  0.833333  0.419474   \n",
       "1                      Support Vector Machines:  0.805556  0.480189   \n",
       "\n",
       "   Cross-Validation  \n",
       "5          0.927146  \n",
       "6          0.927273  \n",
       "0          0.859722  \n",
       "2          0.893813  \n",
       "4          0.921465  \n",
       "3          0.736237  \n",
       "1          0.663131  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generem un DF per comparar les dades obtingudes en l'analisis dels diferents models\n",
    "\n",
    "Comparatiu_Accuracy_df = pd.DataFrame(data = [('DecisionTreeClassifier:', accuracy_DTC_wine, Loss_DTC_wine, cv_DTC_wine.mean()),\n",
    "                                              ('Support Vector Machines:', accuracy_SVM_wine, Loss_SVM_wine, cv_SVM_wine.mean()),\n",
    "                                              ('DecisionTreeClassifier Entrenat:',accuracy_DTC_wine_2, Loss_DTC_wine_2, cv_DTC_wine_2.mean()),\n",
    "                                              ('Support Vector Machines Entrenat:', accuracy_SVM_wine_2, Loss_SVM_wine_2, cv_SVM_wine_2.mean()),\n",
    "                                              ('DecisionTreeClassifier Entrenat Normalitzat:', accuracy_DTC_wine_2_norm, Loss_DTC_wine_2_norm, cv_DTC_wine_2_norm.mean()),\n",
    "                                              ('DecisionTreeClassifier Entrenat Selecció:',accuracy_DTC_wine_2_sel, Loss_DTC_wine_2_sel, cv_DTC_wine_2_sel.mean()),\n",
    "                                              ('DecisionTreeClassifier Entrenat Mostreig:', accuracy_DTC_wine_2_mostreig, Loss_DTC_wine_2_mostreig, cv_DTC_wine_2_mostreig.mean())],\n",
    "                                      columns= ['model', 'Accuracy', 'Log_Loss', 'Cross-Validation'])\n",
    "\n",
    "Comparatiu_Accuracy_df = Comparatiu_Accuracy_df.sort_values(by=['Accuracy'], ascending=False)\n",
    "\n",
    "Comparatiu_Accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52fb5b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred.DTC Class 1</th>\n",
       "      <th>Pred.DTC Class 2</th>\n",
       "      <th>Pred.DTC Class 3</th>\n",
       "      <th>Pred.SVM Class 1</th>\n",
       "      <th>Pred.SVM Class 2</th>\n",
       "      <th>Pred.SVM Class 3</th>\n",
       "      <th>Pred.DTC_2 Class 1</th>\n",
       "      <th>Pred.DTC_2 Class 2</th>\n",
       "      <th>Pred.DTC_2 Class 3</th>\n",
       "      <th>Pred.SVM_2 Class 1</th>\n",
       "      <th>...</th>\n",
       "      <th>Pred.SVM_2 Class 3</th>\n",
       "      <th>Pred.NORM Class 1</th>\n",
       "      <th>Pred.NORM Class 2</th>\n",
       "      <th>Pred.NORM Class 3</th>\n",
       "      <th>Pred.SEL Class 1</th>\n",
       "      <th>Pred.SEL Class 2</th>\n",
       "      <th>Pred.SEL Class 3</th>\n",
       "      <th>Pred.MOSTREIG Class 1</th>\n",
       "      <th>Pred.MOSTREIG Class 2</th>\n",
       "      <th>Pred.MOSTREIG Class 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 2</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Pred.DTC Class 1  Pred.DTC Class 2  Pred.DTC Class 3  \\\n",
       "Class 1                13                 1                 0   \n",
       "Class 2                 0                14                 0   \n",
       "Class 3                 1                 0                 7   \n",
       "\n",
       "         Pred.SVM Class 1  Pred.SVM Class 2  Pred.SVM Class 3  \\\n",
       "Class 1                14                 0                 0   \n",
       "Class 2                 0                11                 3   \n",
       "Class 3                 0                 4                 4   \n",
       "\n",
       "         Pred.DTC_2 Class 1  Pred.DTC_2 Class 2  Pred.DTC_2 Class 3  \\\n",
       "Class 1                  13                   1                   0   \n",
       "Class 2                   0                  14                   0   \n",
       "Class 3                   0                   1                   7   \n",
       "\n",
       "         Pred.SVM_2 Class 1  ...  Pred.SVM_2 Class 3  Pred.NORM Class 1  \\\n",
       "Class 1                  14  ...                   0                 13   \n",
       "Class 2                   0  ...                   4                  0   \n",
       "Class 3                   1  ...                   6                  0   \n",
       "\n",
       "         Pred.NORM Class 2  Pred.NORM Class 3  Pred.SEL Class 1  \\\n",
       "Class 1                  1                  0                14   \n",
       "Class 2                 14                  0                 0   \n",
       "Class 3                  1                  7                 1   \n",
       "\n",
       "         Pred.SEL Class 2  Pred.SEL Class 3  Pred.MOSTREIG Class 1  \\\n",
       "Class 1                 0                 0                     11   \n",
       "Class 2                14                 0                      0   \n",
       "Class 3                 0                 7                      0   \n",
       "\n",
       "         Pred.MOSTREIG Class 2  Pred.MOSTREIG Class 3  \n",
       "Class 1                      1                      0  \n",
       "Class 2                      6                      0  \n",
       "Class 3                      0                      7  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generem un DF per comparar totes les Confusion Matrix obtingudes\n",
    "\n",
    "cm_DataFrames = [cm_DTC_wine_DF, cm_SVM_wine_DF, cm_DTC_wine_2_DF, cm_SVM_wine_2_DF, cm_DTC_wine_2_norm_DF, cm_DTC_wine_2_sel_DF, cm_DTC_wine_2_mostreig_DF]\n",
    "\n",
    "cm_comparatiu = pd.concat(cm_DataFrames, axis = 1)\n",
    "\n",
    "cm_comparatiu\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aec39e",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97e60e4",
   "metadata": {},
   "source": [
    "#### Comentaris RESUM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f005e93",
   "metadata": {},
   "source": [
    "Havent realitzat i analitzat els models de classificació 'DecisionTreeClassifier' (DTC) i 'SupportVectorMachines' (SVM) hem obtingut els següents resultats:\n",
    "\n",
    "- Models Inicials:\n",
    "  - Accuracy: El 'DTC' inicial obté una 'accuracy' (percentatge de prediccions correctes) del 94% per sobre de 80% de la 'accuracy'obtinguda amb el model 'SVM'. Analitzant la 'accuracy' podriem dir que el model més ajustat és el DTC.\n",
    "  - Confusion Matrix: El 'DTC' inicial acumula 2 errors en les prediccions respecte els 7 errors del model 'SVM'. Com en el cas de l'analisi de l'accurcy, podriem dir que analitzant la confusion matrix, el model 'DTC' inicial és més precís.\n",
    "  - Logistic Loss: En aquest cas, al 'DTC' inicial obtenim una 'log_loss' superior a la obtinguda amb el model 'SVM' inicial. En aques cas, podríem dir que les prediccions del model 'SVM' inicial podríen ser millors respecte a les del model 'DTC' inicial.\n",
    "  - Cross validation: En aquest cas el 'DTC' inicial obté un valor de CV molt superior al obtingut al model 'SVM' incial. Podríem dir que en aquest cas el model 'DTC' inicial és més precís.\n",
    "        \n",
    "- Models Entrenats:\n",
    "    - Entrenant els models, el model 'DTC' només obté una petita millora en el Cross Validation, mentre que el model 'SVM' obté millores en els tots els analisis, inclús en la confusion matrix que pasa del 7 errors als 6 errors.\n",
    "    - Malgrat no haver obtingut millores substancials després d'entrenar el model 'DTC' inicial, el model DTC entrenat a priori continua sent més ajustat que el model 'SVM' entrenat.\n",
    "        \n",
    "- Millora dels rsultats:\n",
    "    - Hem realitzat processos de millora sobre el model 'DTC' entrenat\n",
    "     - Normalització: en principi no es millora el model entrenat\n",
    "     - Selecció millors 'features': en aquest cas s'aconsegueix aumentar el percentatge 'accuracy' fins al 97% i reduir substancialment la 'log_loss'. Malgrat la CV es manté, s'aconsegueix reduir a 1 error les predccions a la confusion matrix.\n",
    "     - Mostreig: s'aconsegueix aumnetar sensiblement l'accuracy i la CV, i es redueix en menor quantia la log_loss. S'aconsegueix reduir també els errors de predicció de la confusion matrix, reduint també a 1 error.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a4a8f1",
   "metadata": {},
   "source": [
    "#### Conclusió\n",
    "\n",
    "Havent realitzat, entrenat, millorat i analitzat els diferents models, es considera que possiblement el millor model per realitzar les prediccions d'indentificació de les diferents Classes és el model __DecisionTreeClassifier Entrenat Selecció__\n",
    "\n",
    "- __Model DTC entrenat amb paràmetres i millorat amb la selecció de les millors 'features'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf5330d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
